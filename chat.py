# ==========================================
# HO T√äN: ƒê·ªó Th·ªã H·ªìng ƒêi·ªáp
# MSSV: 23103014
# ƒê·ªí √ÅN: Chatbot Dynamic Router - TTN University
# NG√ÄY N·ªòP: 21/12/2025
# Copyright ¬© 2025. All rights reserved.
# ==========================================

# ============================================
#  CHATBOT 4-B∆Ø·ªöC ‚Äì HI·ªÇU NGHƒ®A, KH√îNG B·ªäA
#  PHI√äN B·∫¢N T·ªêI ∆ØU RAM
# ============================================

import os
import numpy as np
from sentence_transformers import SentenceTransformer
import requests
import json
import re
import time
import random
import gc  # ‚úÖ Garbage collector
from dotenv import load_dotenv


# Load .env
# Load .env
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ENV_PATH = os.path.join(BASE_DIR, "rag", ".env")

try:
    if os.path.exists(ENV_PATH):
        load_dotenv(ENV_PATH, override=True)
    else:
        load_dotenv()
except Exception:
    pass

ZIPUR_API_KEY = os.getenv("ZIPUR_API_KEY")
ZIPUR_MODEL = os.getenv("ZIPUR_MODEL", "glm-4-plus")

FALLBACK_MSG = "Hi·ªán t·∫°i th∆∞ vi·ªán ch∆∞a c√≥ th√¥ng tin ch√≠nh x√°c cho c√¢u n√†y. B·∫°n m√¥ t·∫£ r√µ h∆°n gi√∫p m√¨nh nh√©."

# ============================================
#  EMBEDDING MODEL - LAZY LOAD + AUTO CLEANUP
# ============================================
embed_model = None
last_model_use = 0
MODEL_TIMEOUT = 300  # ‚úÖ Gi·∫£i ph√≥ng model sau 5 ph√∫t kh√¥ng d√πng

def get_model():
    global embed_model, last_model_use
    
    if embed_model is not None:
        last_model_use = time.time()
        return embed_model
    try:
        print("üîÑ ƒêang load model BAAI/bge-m3...")
        embed_model = SentenceTransformer("BAAI/bge-m3")
        print("‚úÖ Load BAAI/bge-m3 th√†nh c√¥ng!")
    except Exception as e:
        print(f"‚ö† L·ªói load BAAI/bge-m3: {e}")
        print("üîÑ ƒêang d√πng fallback model keepitreal/vietnamese-sbert...")
        embed_model = SentenceTransformer("keepitreal/vietnamese-sbert")
        print("‚úÖ Load fallback th√†nh c√¥ng!")
    last_model_use = time.time()
    return embed_model
#normalized_text = normalize(text)
def cleanup_model_if_idle():
    """‚úÖ Gi·∫£i ph√≥ng model n·∫øu kh√¥ng d√πng l√¢u"""
    global embed_model, last_model_use
    if embed_model is not None and (time.time() - last_model_use) > MODEL_TIMEOUT:
        print("üßπ Gi·∫£i ph√≥ng embedding model (idle qu√° l√¢u)...")
        del embed_model
        embed_model = None
        gc.collect()

# ============================================
#  TEXT NORMALIZE
# ============================================
def normalize(x: str) -> str:
    return " ".join(x.lower().strip().split())

# ============================================
#  LLM CALL - T·ªêI ∆ØU H√ìA
# ============================================
def llm(prompt: str, temp: float = 0.15, n: int = 1024) -> str:
    """
    G·ªçi Zhipu AI API v·ªõi retry logic
    ‚úÖ Gi·∫£m timeout, gi·∫£m max_tokens m·∫∑c ƒë·ªãnh
    """
    if not ZIPUR_API_KEY:
        return ""

    headers = {
        "Authorization": f"Bearer {ZIPUR_API_KEY}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": ZIPUR_MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": temp,
        "max_tokens": n,
    }

    max_retries = 2  # ‚úÖ Gi·∫£m t·ª´ 3 xu·ªëng 2
    base_delay = 1   # ‚úÖ Gi·∫£m t·ª´ 2s xu·ªëng 1s

    for attempt in range(max_retries):
        try:
            resp = requests.post(
                "https://open.bigmodel.cn/api/paas/v4/chat/completions",
                headers=headers,
                json=payload,
                timeout=20  # ‚úÖ Gi·∫£m t·ª´ 30s xu·ªëng 20s
            )
            
            if resp.status_code == 200:
                data = resp.json()
                result = data["choices"][0]["message"]["content"].strip()
                del data  # ‚úÖ Gi·∫£i ph√≥ng response data
                return result
            
            if resp.status_code == 429:
                wait_time = base_delay * (2 ** attempt) + random.uniform(0, 0.5)
                print(f"‚ö† Zhipu AI qu√° t·∫£i (429). ƒêang ch·ªù {wait_time:.1f}s...")
                time.sleep(wait_time)
                continue
                
            print(f"‚ö† L·ªói Zhipu AI {resp.status_code}: {resp.text}")
            return ""

        except Exception as e:
            print(f"‚ö† L·ªói g·ªçi Zhipu AI: {e}")
            return ""
    
    return ""

# ============================================
#  CONNECT TO QDRANT - LAZY INIT
# ============================================
from qdrant_client import QdrantClient

QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")

qdrant_client = None

def get_qdrant_client():
    """‚úÖ Lazy initialization cho Qdrant client"""
    global qdrant_client
    if qdrant_client is None:
        print("üîó K·∫øt n·ªëi t·ªõi Qdrant...")
        if QDRANT_API_KEY:
            qdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)
        else:
            qdrant_client = QdrantClient(url=QDRANT_URL)
        
        try:
            collections = qdrant_client.get_collections().collections
            collection_names = [c.name for c in collections]
            print(f"‚úÖ ƒê√£ k·∫øt n·ªëi Qdrant: {len(collections)} collections ({', '.join(collection_names)})")
        except Exception as e:
            print(f"‚ùå L·ªói k·∫øt n·ªëi Qdrant: {e}")
    
    return qdrant_client

# ============================================
#  ROUTER - T·ªêI ∆ØU H√ìA
# ============================================

def is_greeting(text: str) -> bool:
    t = text.lower().strip()
    greet_words = ["xin ch√†o", "ch√†o b·∫°n", "ch√†o ad", "hello", "hi", "alo"]
    return any(w in t for w in greet_words)



# ============================================
#  REWRITE - T·ªêI ∆ØU H√ìA
# ============================================

# ============================================
def rerank_with_llm(user_q: str, candidates: list, context_str: str = ""):
    """‚úÖ Gi·∫£m max_tokens t·ª´ 128 xu·ªëng 64"""
    if not candidates:
        return None

    # ‚úÖ Ch·ªâ rerank top 3 (gi·∫£m t·ª´ 5) ƒë·ªÉ nhanh h∆°n
    top_candidates = candidates[:3]
    
    block = ""
    for i, c in enumerate(top_candidates, start=1):
        block += f"{i}. [{c['category']}] {c['answer']}\n"
    #ti√™m tr√≠ nh·ªõ
    
    context_section = ""
    if context_str:
        context_section = f"\nL·ªãch s·ª≠ h·ªôi tho·∫°i:\n{context_str}\n"
    prompt = f"""
B·∫°n l√† chuy√™n gia t∆∞ v·∫•n th√¥ng minh.
Nhi·ªám v·ª•: T√¨m c√¢u tr·∫£ l·ªùi PH√ô H·ª¢P NH·∫§T cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng trong danh s√°ch b√™n d∆∞·ªõi.
{context_section}
C√¢u h·ªèi: "{user_q}"

Danh s√°ch ·ª©ng vi√™n:
{block}

H∆Ø·ªöNG D·∫™N T∆Ø DUY:
- H√£y hi·ªÉu √ù NGHƒ®A c·ªßa c√¢u h·ªèi (kh√¥ng ch·ªâ b·∫Øt t·ª´ kh√≥a).
- N·∫øu c√≥ l·ªãch s·ª≠ h·ªôi tho·∫°i, s·ª≠ d·ª•ng context ƒë·ªÉ hi·ªÉu c√¢u h·ªèi t·ªët h∆°n.
- V√≠ d·ª•: H·ªèi "Fanpage" th√¨ c√¢u ch·ª©a "Facebook" l√† ƒë√∫ng. H·ªèi "Quy tr√¨nh" th√¨ c√¢u h∆∞·ªõng d·∫´n c√°c b∆∞·ªõc l√† ƒë√∫ng.
- N·∫øu c√¢u h·ªèi t√¨m "ƒê·ªãa ƒëi·ªÉm" (·ªü ƒë√¢u), h√£y ch·ªçn c√¢u ch·ª©a th√¥ng tin v·ªã tr√≠.
- N·∫øu c√¢u h·ªèi t√¨m "Danh s√°ch" (g·ªìm nh·ªØng g√¨), h√£y ch·ªçn c√¢u li·ªát k√™ ƒë·∫ßy ƒë·ªß nh·∫•t.

Y√äU C·∫¶U:
- N·∫øu t√¨m th·∫•y c√¢u tr·∫£ l·ªùi ph√π h·ª£p: Tr·∫£ v·ªÅ S·ªê TH·ª® T·ª∞ (v√≠ d·ª•: 1, 2...).
- N·∫øu kh√¥ng c√≥ c√¢u n√†o kh·ªõp: Tr·∫£ v·ªÅ 0.

Ch·ªâ tr·∫£ v·ªÅ 1 con s·ªë duy nh·∫•t.
"""
    out = llm(prompt, temp=0.1, n=32).strip()  # ‚Üê Gi·∫£m t·ª´ 64 xu·ªëng 32

    match = re.search(r'\d+', out)
    if match:
        idx = int(match.group()) - 1
        if 0 <= idx < len(top_candidates):
            return top_candidates[idx]

    # Fallback: tin top 1 n·∫øu score r·∫•t cao
    if top_candidates and top_candidates[0]['score'] > 0.45:
        print(f"[Rerank] LLM t·ª´ ch·ªëi, nh∆∞ng Top 1 score cao ({top_candidates[0]['score']:.2f}) -> Ch·ªçn Top 1.")
        return top_candidates[0]

    return None



#  MAIN PROCESS - DYNAMIC & AUTOMATED
# ============================================
def process_message(text: str, history: list = None, image_path: str = None) -> str:
    """
    DYNAMIC VERSION + Multi-step Reasoning + Conversation Memory
    - Router ng·ªØ nghƒ©a (Vector + LLM CoT)
    - Clarification (h·ªèi l·∫°i khi m∆° h·ªì)
    - Search theo collection
    - Humanize answer (ch·ªâ h·ªçc t·ª´ C√ÇU TR·∫¢ L·ªúI)
    - Conversation memory (nh·ªõ 2-3 c√¢u tr∆∞·ªõc)
    """
    print("[CHAT.PY] ƒê√É G·ªåI N√ÉO (Dynamic Reasoning Mode)")

    if not text.strip():
        return "Xin ch√†o üëã B·∫°n mu·ªën h·ªèi th√¥ng tin g√¨ trong th∆∞ vi·ªán?"
    
    # B∆∞·ªõc 1  l·∫•y l·ªãch s·ª≠ g·∫ßn nh·∫•t
    context_str = ""
    if history:
        context_lines = []
        for user_msg, bot_msg in history:
            context_lines.append(f"User: {user_msg}")
            context_lines.append(f"Bot: {bot_msg}")
        context_str = "\n".join(context_lines)
        print(f"[CONTEXT] Using {len(history)} previous messages")
# l·∫•y c·∫∑p ƒë√≥ng g√≥i1 ph√≠a tr√™n 196 ph√≠a d∆∞·ªõi 296
    try:
        # Import dynamic tools (ƒë√£ s·ª≠a ·ªü tr√™n)
        from chat_dynamic_router import (
            reason_and_route,
            search_dynamic,
            get_collections_with_descriptions,
            humanize_answer,
            

        )

        # ‚úÖ L·∫•y model (lazy load)
        model = get_model()

        # chu·∫©n ho√° v√† t·∫°o vector
        normalized_text = normalize(text)
        q_vec = model.encode(normalized_text, normalize_embeddings=True)
        #reason_and_route
        if is_greeting(text) and len(text.split()) <= 4:
            collections = get_collections_with_descriptions()
            collection_names = ", ".join(
                [n.upper() for n in list(collections.keys())[:3]]
            )
            return (
                f"Xin ch√†o! T√¥i l√† tr·ª£ l√Ω ·∫£o. B·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ: "
                f"{collection_names} ho·∫∑c b·∫•t c·ª© th√¥ng tin n√†o kh√°c..."
            )

        # B2: Multi-step Reasoning Router (CoT + Clarification)
       #N∆°i chu·∫©n b·ªã d·ªØ li·ªáu
        router_question = text
        if context_str:
            router_question = f"{text}\n\n[L·ªãch s·ª≠ g·∫ßn ƒë√¢y:\n{context_str}]"
        router_result = reason_and_route(router_question, q_vec, llm, model)

        # N·∫øu c·∫ßn h·ªèi l·∫°i ‚Üí tr·∫£ lu√¥n c√¢u h·ªèi clarify (kh√¥ng search)
        if router_result.needs_clarification and router_result.clarification_question:
            print("[PROCESS] Clarification required ‚Üí h·ªèi l·∫°i ng∆∞·ªùi d√πng.")
            return router_result.clarification_question


        # B∆Ø·ªöC 6 ‚Äì Search ƒë√∫ng collection (c√≥ l·ªçc ng√†nh n·∫øu c·∫ßn)
        rewritten = router_result.rewritten_question or text

    
        q_vec_search = model.encode(
            normalize(rewritten), normalize_embeddings=True
        )
      

        # B5: Search v√†o knowledge_base, filter theo collection n·∫øu c√≥
        collection_name = router_result.target_collection or "global"
        print(f"[PROCESS] Search in collection: {collection_name}")
        
        # ‚úÖ B5a: S·ª≠ d·ª•ng Dynamic Filter t·ª´ Router (n·∫øu c√≥)
    
        candidates = search_dynamic(
            collection_name, 
            q_vec_search, 
            top_k=10, 
            
        )

        if not candidates:
            print("[DEBUG] ‚ùå Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o.")
            return "Xin l·ªói, t√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong c∆° s·ªü d·ªØ li·ªáu."

        print(f"[DEBUG] Found {len(candidates)} candidates.")
        for c in candidates:
            print(
                f"  - [{c['score']:.4f}] {c['answer'][:80]}... (Cat: {c['category']})"
            )

        # B6a: D√πng LLM ƒë·ªÉ hi·ªÉu user mu·ªën bao nhi√™u k·∫øt qu·∫£
        extract_prompt = f"""
Ph√¢n t√≠ch c√¢u h·ªèi sau v√† tr·∫£ l·ªùi:

C√¢u h·ªèi: "{text}"

H·ªèi:
1. User c√≥ th·ª±c s·ª± mu·ªën h·ªèi danh s√°ch NHI·ªÄU k·∫øt qu·∫£ kh√¥ng (v√≠ d·ª•: "li·ªát k√™", "c√°c lo·∫°i", "nh·ªØng cu·ªën", "top 5")? (c√≥/kh√¥ng)
2. N·∫øu c√≥, user mu·ªën bao nhi√™u k·∫øt qu·∫£? (tr·∫£ s·ªë, n·∫øu kh√¥ng r√µ th√¨ tr·∫£ 1)

Ch·ªâ tr·∫£ l·ªùi theo format: <c√≥/kh√¥ng>|<s·ªë>

V√≠ d·ª•:
- "G·ª£i √Ω c√°c s√°ch v·ªÅ Python" ‚Üí c√≥|3
- "Cho t√¥i 5 cu·ªën v·ªÅ AI" ‚Üí c√≥|5
- "S√°ch Python gi√° bao nhi√™u?" ‚Üí kh√¥ng|1
- "S√°ch python" ‚Üí kh√¥ng|1
- "Th√¥ng tin v·ªÅ s√°ch Java" ‚Üí kh√¥ng|1
"""
        
        try:
            llm_response = llm(extract_prompt, temp=0.1, n=20).strip()
            parts = llm_response.split('|')
            
            if len(parts) == 2 and parts[0].lower() == 'c√≥':
                try:
                    requested_count = int(parts[1])
                    print(f"[DEBUG] üî¢ LLM ph√°t hi·ªán: User mu·ªën {requested_count} k·∫øt qu·∫£")
                    
                    # L·∫•y ƒë√∫ng s·ªë l∆∞·ª£ng user y√™u c·∫ßu (t·ªëi ƒëa 10)
                    actual_count = min(requested_count, len(candidates), 10)
                    top_n = candidates[:actual_count * 2]  # L·∫•y g·∫•p ƒë√¥i ƒë·ªÉ l·ªçc
                    
                    # ‚úÖ LLM Filter: L·ªçc ch·ªâ gi·ªØ k·∫øt qu·∫£ li√™n quan
                    filter_prompt = f"""
C√¢u h·ªèi: "{text}"
Danh s√°ch k·∫øt qu·∫£:
{chr(10).join([f"{i+1}. {c['answer'][:200]}" for i, c in enumerate(top_n)])}

NHI·ªÜM V·ª§: Ch·ªçn {requested_count} k·∫øt qu·∫£ TH·ª∞C S·ª∞ LI√äN QUAN ƒë·∫øn c√¢u h·ªèi.

QUY T·∫ÆC NGHI√äM NG·∫∂T:
- N·∫øu h·ªèi v·ªÅ "c√¥ng ngh·ªá th√¥ng tin" ‚Üí CH·ªà ch·ªçn s√°ch v·ªÅ l·∫≠p tr√¨nh, AI, d·ªØ li·ªáu, m√°y t√≠nh
- LO·∫†I B·ªé s√°ch v·ªÅ: ng√¥n ng·ªØ, to√°n h·ªçc c∆° b·∫£n, v·∫≠t l√Ω, h√≥a h·ªçc (tr·ª´ khi c√¢u h·ªèi y√™u c·∫ßu)
- ∆Øu ti√™n s√°ch c√≥ t·ª´ kh√≥a CH√çNH X√ÅC kh·ªõp v·ªõi c√¢u h·ªèi

Tr·∫£ v·ªÅ danh s√°ch s·ªë th·ª© t·ª± (v√≠ d·ª•: 2,5,7), KH√îNG gi·∫£i th√≠ch:
"""
                    try:
                        filter_response = llm(filter_prompt, temp=0.1, n=30).strip()
                        selected_indices = [int(x.strip())-1 for x in filter_response.split(',') if x.strip().isdigit()]
                        selected_candidates = [top_n[i] for i in selected_indices if 0 <= i < len(top_n)]
                        
                        if selected_candidates:
                            top_n = selected_candidates[:requested_count]
                            print(f"[DEBUG] üîç LLM filtered: Gi·ªØ {len(top_n)} k·∫øt qu·∫£ li√™n quan")
                        else:
                            top_n = top_n[:requested_count]  # Fallback
                    except:
                        top_n = top_n[:requested_count]  # Fallback n·∫øu filter l·ªói
                    
                    combined_answer = "\n\n".join([
                        f"{i+1}. {c['answer']}" 
                        for i, c in enumerate(top_n)
                    ])
                    print(f"[DEBUG] ‚úÖ Tr·∫£ v·ªÅ {actual_count} k·∫øt qu·∫£")
                    print(f"[DEBUG] üìù Raw answer (before humanize):")
                    print(combined_answer)
                    print("[DEBUG] ==================")
                    final_ans = humanize_answer(text, combined_answer)
                    print(f"[DEBUG] üé® After humanize:")
                    print(final_ans)
                    print("[DEBUG] ==================")
                    return final_ans
                except ValueError:
                    pass
        except Exception as e:
            print(f"[DEBUG] ‚ö†Ô∏è LLM extract failed: {e}, falling back to single-result rerank")

        # B6b: Rerank v·ªõi LLM (Ch·ªçn c√¢u tr·∫£ l·ªùi ph√π h·ª£p nh·∫•t) - Ch·ªâ khi h·ªèi 1 c√¢u c·ª• th·ªÉ
        best_cand = rerank_with_llm(rewritten, candidates, context_str=context_str)

        if not best_cand:
            if candidates and candidates[0]["score"] > 0.35:
                best_cand = candidates[0]
                print(
                    "[DEBUG] ‚ö†Ô∏è Rerank t·ª´ ch·ªëi, nh∆∞ng l·∫•y Top 1 do score ·ªïn."
                )
            else:
                print("[DEBUG] ‚ùå Rerank t·ª´ ch·ªëi t·∫•t c·∫£.")
                return (
                    "Xin l·ªói, t√¥i t√¨m th·∫•y m·ªôt s·ªë th√¥ng tin nh∆∞ng c√≥ v·∫ª kh√¥ng kh·ªõp v·ªõi c√¢u h·ªèi c·ªßa b·∫°n."
                )
        else:
            print(
                f"[DEBUG] ‚úÖ Rerank ch·ªçn: {best_cand['answer'][:80]}..."
            )

        # B7: HUMANIZE ANSWER (vi·∫øt l·∫°i t·ª± nhi√™n)
        raw_answer = best_cand["answer"]
        final_ans = humanize_answer(text, raw_answer)
        return final_ans

    except Exception as e:
        print(f"[PROCESS] ‚ùå Error: {e}")
        import traceback

        traceback.print_exc()
        return "Xin l·ªói, h·ªá th·ªëng ƒëang g·∫∑p l·ªói x·ª≠ l√Ω. Vui l√≤ng th·ª≠ l·∫°i sau."

    finally:
        gc.collect()
        cleanup_model_if_idle()
if __name__ == "__main__":
    print("ü§ñ Chatbot 4-B∆Ø·ªöC (Phi√™n b·∫£n T·ªêI ∆ØU RAM) ƒë√£ s·∫µn s√†ng!")
    while True:
        q = input("\nB·∫°n: ")
        if q.lower() in ["quit", "bye", "exit", "tho√°t"]:
            print("H·∫πn g·∫∑p l·∫°i b·∫°n ·ªü th∆∞ vi·ªán nh√©! üìö")
            break
        print("Bot:", process_message(q))

# User question
#  ‚Üí embed
#  ‚Üí router ch·ªçn collection
#  
#  ‚Üí rewrite c√¢u h·ªèi
#  ‚Üí embed l·∫°i
#  ‚Üí search_dynamic  ‚Üê GI·ªÆ D√íNG N√ÄY
#  ‚Üí rerank_with_llm
#  ‚Üí humanize_answer

