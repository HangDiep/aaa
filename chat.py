# ============================================
#  CHATBOT 4-B∆Ø·ªöC ‚Äì HI·ªÇU NGHƒ®A, KH√îNG B·ªäA
#  PHI√äN B·∫¢N T·ªêI ∆ØU RAM
# ============================================

import os
import numpy as np
from sentence_transformers import SentenceTransformer
import requests
import json
import re
import time
import random
import gc  # ‚úÖ Garbage collector
from dotenv import load_dotenv

# Load .env
ENV_PATH = r"D:\HTML\a_Copy\rag\.env"
try:
    if os.path.exists(ENV_PATH):
        load_dotenv(ENV_PATH, override=True)
    else:
        load_dotenv()
except Exception:
    pass

GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL = os.getenv("GROQ_MODEL", "glm-4-plus")

FALLBACK_MSG = "Hi·ªán t·∫°i th∆∞ vi·ªán ch∆∞a c√≥ th√¥ng tin ch√≠nh x√°c cho c√¢u n√†y. B·∫°n m√¥ t·∫£ r√µ h∆°n gi√∫p m√¨nh nh√©."

# ============================================
#  EMBEDDING MODEL - LAZY LOAD + AUTO CLEANUP
# ============================================
embed_model = None
last_model_use = 0
MODEL_TIMEOUT = 300  # ‚úÖ Gi·∫£i ph√≥ng model sau 5 ph√∫t kh√¥ng d√πng

def get_model():
    global embed_model, last_model_use
    
    if embed_model is not None:
        last_model_use = time.time()
        return embed_model
    
    try:
        print("üîÑ ƒêang load model BAAI/bge-m3...")
        embed_model = SentenceTransformer("BAAI/bge-m3")
        print("‚úÖ Load BAAI/bge-m3 th√†nh c√¥ng!")
    except Exception as e:
        print(f"‚ö† L·ªói load BAAI/bge-m3: {e}")
        print("üîÑ ƒêang d√πng fallback model keepitreal/vietnamese-sbert...")
        embed_model = SentenceTransformer("keepitreal/vietnamese-sbert")
        print("‚úÖ Load fallback th√†nh c√¥ng!")
    
    last_model_use = time.time()
    return embed_model

def cleanup_model_if_idle():
    """‚úÖ Gi·∫£i ph√≥ng model n·∫øu kh√¥ng d√πng l√¢u"""
    global embed_model, last_model_use
    if embed_model is not None and (time.time() - last_model_use) > MODEL_TIMEOUT:
        print("üßπ Gi·∫£i ph√≥ng embedding model (idle qu√° l√¢u)...")
        del embed_model
        embed_model = None
        gc.collect()

# ============================================
#  TEXT NORMALIZE
# ============================================
def normalize(x: str) -> str:
    return " ".join(x.lower().strip().split())

# ============================================
#  LLM CALL - T·ªêI ∆ØU H√ìA
# ============================================
def llm(prompt: str, temp: float = 0.15, n: int = 1024) -> str:
    """
    G·ªçi Zhipu AI API v·ªõi retry logic
    ‚úÖ Gi·∫£m timeout, gi·∫£m max_tokens m·∫∑c ƒë·ªãnh
    """
    if not GROQ_API_KEY:
        return ""

    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": GROQ_MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": temp,
        "max_tokens": n,
    }

    max_retries = 2  # ‚úÖ Gi·∫£m t·ª´ 3 xu·ªëng 2
    base_delay = 1   # ‚úÖ Gi·∫£m t·ª´ 2s xu·ªëng 1s

    for attempt in range(max_retries):
        try:
            resp = requests.post(
                "https://open.bigmodel.cn/api/paas/v4/chat/completions",
                headers=headers,
                json=payload,
                timeout=20  # ‚úÖ Gi·∫£m t·ª´ 30s xu·ªëng 20s
            )
            
            if resp.status_code == 200:
                data = resp.json()
                result = data["choices"][0]["message"]["content"].strip()
                del data  # ‚úÖ Gi·∫£i ph√≥ng response data
                return result
            
            if resp.status_code == 429:
                wait_time = base_delay * (2 ** attempt) + random.uniform(0, 0.5)
                print(f"‚ö† Zhipu AI qu√° t·∫£i (429). ƒêang ch·ªù {wait_time:.1f}s...")
                time.sleep(wait_time)
                continue
                
            print(f"‚ö† L·ªói Zhipu AI {resp.status_code}: {resp.text}")
            return ""

        except Exception as e:
            print(f"‚ö† L·ªói g·ªçi Zhipu AI: {e}")
            return ""
    
    return ""

# ============================================
#  CONNECT TO QDRANT - LAZY INIT
# ============================================
from qdrant_client import QdrantClient

QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")

qdrant_client = None

def get_qdrant_client():
    """‚úÖ Lazy initialization cho Qdrant client"""
    global qdrant_client
    if qdrant_client is None:
        print("üîó K·∫øt n·ªëi t·ªõi Qdrant...")
        if QDRANT_API_KEY:
            qdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)
        else:
            qdrant_client = QdrantClient(url=QDRANT_URL)
        
        try:
            collections = qdrant_client.get_collections().collections
            collection_names = [c.name for c in collections]
            print(f"‚úÖ ƒê√£ k·∫øt n·ªëi Qdrant: {len(collections)} collections ({', '.join(collection_names)})")
        except Exception as e:
            print(f"‚ùå L·ªói k·∫øt n·ªëi Qdrant: {e}")
    
    return qdrant_client

# ============================================
#  ROUTER - T·ªêI ∆ØU H√ìA
# ============================================

def is_greeting(text: str) -> bool:
    t = text.lower().strip()
    greet_words = ["xin ch√†o", "ch√†o b·∫°n", "ch√†o ad", "hello", "hi", "alo"]
    return any(w in t for w in greet_words)



# ============================================
#  REWRITE - T·ªêI ∆ØU H√ìA
# ============================================

# ============================================
def rerank_with_llm(user_q: str, candidates: list):
    """‚úÖ Gi·∫£m max_tokens t·ª´ 128 xu·ªëng 64"""
    if not candidates:
        return None

    # ‚úÖ Ch·ªâ rerank top 5 thay v√¨ t·∫•t c·∫£
    top_candidates = candidates[:5]
    
    block = ""
    for i, c in enumerate(top_candidates, start=1):
        block += f"{i}. [{c['category']}] {c['answer']}\n"

    prompt = f"""
B·∫°n l√† chuy√™n gia t∆∞ v·∫•n th√¥ng minh.
Nhi·ªám v·ª•: T√¨m c√¢u tr·∫£ l·ªùi PH√ô H·ª¢P NH·∫§T cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng trong danh s√°ch b√™n d∆∞·ªõi.

C√¢u h·ªèi: "{user_q}"

Danh s√°ch ·ª©ng vi√™n:
{block}

H∆Ø·ªöNG D·∫™N T∆Ø DUY:
- H√£y hi·ªÉu √ù NGHƒ®A c·ªßa c√¢u h·ªèi (kh√¥ng ch·ªâ b·∫Øt t·ª´ kh√≥a).
- V√≠ d·ª•: H·ªèi "Fanpage" th√¨ c√¢u ch·ª©a "Facebook" l√† ƒë√∫ng. H·ªèi "Quy tr√¨nh" th√¨ c√¢u h∆∞·ªõng d·∫´n c√°c b∆∞·ªõc l√† ƒë√∫ng.
- N·∫øu c√¢u h·ªèi t√¨m "ƒê·ªãa ƒëi·ªÉm" (·ªü ƒë√¢u), h√£y ch·ªçn c√¢u ch·ª©a th√¥ng tin v·ªã tr√≠.
- N·∫øu c√¢u h·ªèi t√¨m "Danh s√°ch" (g·ªìm nh·ªØng g√¨), h√£y ch·ªçn c√¢u li·ªát k√™ ƒë·∫ßy ƒë·ªß nh·∫•t.

Y√äU C·∫¶U:
- N·∫øu t√¨m th·∫•y c√¢u tr·∫£ l·ªùi ph√π h·ª£p: Tr·∫£ v·ªÅ S·ªê TH·ª® T·ª∞ (v√≠ d·ª•: 1, 2...).
- N·∫øu kh√¥ng c√≥ c√¢u n√†o kh·ªõp: Tr·∫£ v·ªÅ 0.

Ch·ªâ tr·∫£ v·ªÅ 1 con s·ªë duy nh·∫•t.
"""
    out = llm(prompt, temp=0.1, n=64).strip()

    match = re.search(r'\d+', out)
    if match:
        idx = int(match.group()) - 1
        if 0 <= idx < len(top_candidates):
            return top_candidates[idx]

    # Fallback: tin top 1 n·∫øu score r·∫•t cao
    if top_candidates and top_candidates[0]['score'] > 0.45:
        print(f"[Rerank] LLM t·ª´ ch·ªëi, nh∆∞ng Top 1 score cao ({top_candidates[0]['score']:.2f}) -> Ch·ªçn Top 1.")
        return top_candidates[0]

    return None



#  MAIN PROCESS - DYNAMIC & AUTOMATED
# ============================================
def process_message(text: str) -> str:
    """
    DYNAMIC VERSION + Multi-step Reasoning
    - Router ng·ªØ nghƒ©a (Vector + LLM CoT)
    - Clarification (h·ªèi l·∫°i khi m∆° h·ªì)
    - Search theo collection
    - Humanize answer (ch·ªâ h·ªçc t·ª´ C√ÇU TR·∫¢ L·ªúI)
    """
    print("[CHAT.PY] ƒê√É G·ªåI N√ÉO (Dynamic Reasoning Mode)")

    if not text.strip():
        return "Xin ch√†o üëã B·∫°n mu·ªën h·ªèi th√¥ng tin g√¨ trong th∆∞ vi·ªán?"

    try:
        # Import dynamic tools (ƒë√£ s·ª≠a ·ªü tr√™n)
        from chat_dynamic_router import (
            reason_and_route,
            search_dynamic,
            get_collections_with_descriptions,
            humanize_answer,
        )

        # ‚úÖ L·∫•y model (lazy load)
        model = get_model()

        # B0: T·∫°o vector 1 l·∫ßn duy nh·∫•t
        normalized_text = normalize(text)
        q_vec = model.encode(normalized_text, normalize_embeddings=True)

        # B1: Greeting
        if is_greeting(text) and len(text.split()) <= 4:
            collections = get_collections_with_descriptions()
            collection_names = ", ".join(
                [n.upper() for n in list(collections.keys())[:3]]
            )
            return (
                f"Xin ch√†o! T√¥i l√† tr·ª£ l√Ω ·∫£o. B·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ: "
                f"{collection_names} ho·∫∑c b·∫•t c·ª© th√¥ng tin n√†o kh√°c..."
            )

        # B2: Multi-step Reasoning Router (CoT + Clarification)
        router_result = reason_and_route(text, q_vec, llm, model)

        # N·∫øu c·∫ßn h·ªèi l·∫°i ‚Üí tr·∫£ lu√¥n c√¢u h·ªèi clarify (kh√¥ng search)
        if router_result.needs_clarification and router_result.clarification_question:
            print("[PROCESS] Clarification required ‚Üí h·ªèi l·∫°i ng∆∞·ªùi d√πng.")
            return router_result.clarification_question

        # B3: L·∫•y c√¢u h·ªèi ƒë√£ l√†m r√µ (rewritten_question)
        rewritten = router_result.rewritten_question or text

        # T√πy ch·ªçn: n·∫øu b·∫°n v·∫´n mu·ªën th√™m l·ªõp rewrite_question c≈©
        # rewritten2 = rewrite_question(rewritten)
        # if rewritten2: rewritten = rewritten2

        # B4: Embed l·∫°i cho search
        q_vec_search = model.encode(
            normalize(rewritten), normalize_embeddings=True
        )

        # B5: Search v√†o knowledge_base, filter theo collection n·∫øu c√≥
        collection_name = router_result.target_collection or "global"
        print(f"[PROCESS] Search in collection: {collection_name}")
        candidates = search_dynamic(collection_name, q_vec_search, top_k=10)

        if not candidates:
            print("[DEBUG] ‚ùå Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o.")
            return "Xin l·ªói, t√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong c∆° s·ªü d·ªØ li·ªáu."

        print(f"[DEBUG] Found {len(candidates)} candidates.")
        for c in candidates:
            print(
                f"  - [{c['score']:.4f}] {c['answer'][:80]}... (Cat: {c['category']})"
            )

        # B6: Rerank v·ªõi LLM (Ch·ªçn c√¢u tr·∫£ l·ªùi ph√π h·ª£p nh·∫•t)
        best_cand = rerank_with_llm(rewritten, candidates)

        if not best_cand:
            if candidates and candidates[0]["score"] > 0.35:
                best_cand = candidates[0]
                print(
                    "[DEBUG] ‚ö†Ô∏è Rerank t·ª´ ch·ªëi, nh∆∞ng l·∫•y Top 1 do score ·ªïn."
                )
            else:
                print("[DEBUG] ‚ùå Rerank t·ª´ ch·ªëi t·∫•t c·∫£.")
                return (
                    "Xin l·ªói, t√¥i t√¨m th·∫•y m·ªôt s·ªë th√¥ng tin nh∆∞ng c√≥ v·∫ª kh√¥ng kh·ªõp v·ªõi c√¢u h·ªèi c·ªßa b·∫°n."
                )
        else:
            print(
                f"[DEBUG] ‚úÖ Rerank ch·ªçn: {best_cand['answer'][:80]}..."
            )

        # B7: HUMANIZE ANSWER (ch·ªâ h·ªçc t·ª´ C√ÇU TR·∫¢ L·ªúI)
        raw_answer = best_cand["answer"]
        final_ans = humanize_answer(text, raw_answer)
        return final_ans

    except Exception as e:
        print(f"[PROCESS] ‚ùå Error: {e}")
        import traceback

        traceback.print_exc()
        return "Xin l·ªói, h·ªá th·ªëng ƒëang g·∫∑p l·ªói x·ª≠ l√Ω. Vui l√≤ng th·ª≠ l·∫°i sau."

    finally:
        gc.collect()
        cleanup_model_if_idle()


# ============================================
#  CLI
# ============================================
if __name__ == "__main__":
    print("ü§ñ Chatbot 4-B∆Ø·ªöC (Phi√™n b·∫£n T·ªêI ∆ØU RAM) ƒë√£ s·∫µn s√†ng!")
    while True:
        q = input("\nB·∫°n: ")
        if q.lower() in ["quit", "bye", "exit", "tho√°t"]:
            print("H·∫πn g·∫∑p l·∫°i b·∫°n ·ªü th∆∞ vi·ªán nh√©! üìö")
            break
        print("Bot:", process_message(q))
