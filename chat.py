# ============================================
#  CHATBOT 4-B∆Ø·ªöC ‚Äì HI·ªÇU NGHƒ®A, KH√îNG B·ªäA
#  Router (LLM + Embedding) ‚Üí Rewrite (LLM)
#  ‚Üí Search (Embedding + LLM Rerank) ‚Üí Strict Answer (LLM)
#  Model LLM:  qwen2.5:3b  (ollama)
#  Model Emb:  BAAI/bge-large-en-v1.5
# ============================================

import sqlite3
import requests
import numpy as np
import os
from sentence_transformers import SentenceTransformer

FAQ_DB_PATH = "faq.db"
OLLAMA_URL = "http://127.0.0.1:11434"
MODEL = "qwen2.5:3b"
TIMEOUT = 20

FALLBACK_MSG = "Hi·ªán t·∫°i th∆∞ vi·ªán ch∆∞a c√≥ th√¥ng tin ch√≠nh x√°c cho c√¢u n√†y. B·∫°n m√¥ t·∫£ r√µ h∆°n gi√∫p m√¨nh nh√©."

# ============================================
#  EMBEDDING MODEL (Vietnamese SBERT)
# ============================================
print("ƒêang t·∫£i model embedding (l·∫ßn ƒë·∫ßu s·∫Ω h∆°i l√¢u)...")
try:
    # User suggested BAAI/bge-large-en-v1.5, but BAAI/bge-m3 is SOTA for multilingual/Vietnamese
    embed_model = SentenceTransformer("BAAI/bge-m3")
except Exception as e:
    print(f"‚ö† L·ªói load model embedding: {e}")
    print("ƒêang d√πng fallback model (keepitreal/vietnamese-sbert)...")
    embed_model = SentenceTransformer("keepitreal/vietnamese-sbert")


# ============================================
#  TEXT NORMALIZE ‚Äì NH·∫∏, KH√îNG PH√Å NGHƒ®A
# ============================================
def normalize(x: str) -> str:
    # ch·ªâ lower + trim, kh√¥ng ƒë·ª•ng t·ªõi d·∫•u
    return " ".join(x.lower().strip().split())


# ============================================
#  OLLAMA LLM CALL
# ============================================
def llm(prompt: str, temp: float = 0.15, n: int = 128) -> str:
    try:
        r = requests.post(
            f"{OLLAMA_URL}/api/generate",
            json={
                "model": MODEL,
                "prompt": prompt,
                "stream": False,
                "options": {"temperature": temp, "num_predict": n},
            },
            timeout=TIMEOUT,
        )
        if r.status_code == 200:
            return r.json().get("response", "").strip()
    except Exception:
        pass
    return ""


# ============================================
#  LOAD & EMBED DB
# ============================================
print("ƒêang t·∫£i d·ªØ li·ªáu t·ª´ faq.db...")

if not os.path.exists(FAQ_DB_PATH):
    print(f"‚ùå Kh√¥ng t√¨m th·∫•y file {FAQ_DB_PATH}. H√£y ch·∫°y sync_faq.py tr∆∞·ªõc!")
    # T·∫°o dummy ƒë·ªÉ kh√¥ng crash
    FAQ_TEXTS, BOOK_TEXTS, MAJOR_TEXTS = [], [], []
    FAQ_EMB = np.zeros((0, 768)) # vietnamese-sbert dim is 768
    BOOK_EMB = np.zeros((0, 768))
    MAJOR_EMB = np.zeros((0, 768))
    faq_rows, book_rows, major_rows = [], [], []
else:
    conn = sqlite3.connect(FAQ_DB_PATH)
    cur = conn.cursor()

    # FAQ
    cur.execute(
        "SELECT question, answer, category FROM faq WHERE approved = 1 OR approved IS NULL"
    )
    faq_rows = cur.fetchall()
    
    # UPDATE: Theo y√™u c·∫ßu "hi·ªÉu c√¢u tr·∫£ l·ªùi", ta s·∫Ω embed C√ÇU TR·∫¢ L·ªúI (Answer).
    # Tuy nhi√™n, ƒë·ªÉ AI hi·ªÉu ng·ªØ c·∫£nh t·ªët nh·∫•t, ta n√™n gh√©p c·∫£ Category v√†o (n·∫øu c√≥).
    # V√≠ d·ª•: "Gi·ªù m·ªü c·ª≠a: Th∆∞ vi·ªán m·ªü t·ª´ 7h..." s·∫Ω d·ªÖ t√¨m h∆°n l√† ch·ªâ "Th∆∞ vi·ªán m·ªü t·ª´ 7h..."
    FAQ_TEXTS = []
    for q, a, cat in faq_rows:
        # K·∫øt h·ª£p Category + Answer ƒë·ªÉ t·∫°o th√†nh m·ªôt "kh·ªëi ki·∫øn th·ª©c" (Knowledge Chunk)
        # N·∫øu Answer ƒë√£ ƒë·∫ßy ƒë·ªß √Ω nghƒ©a th√¨ r·∫•t t·ªët.
        content = f"{cat or ''}: {a or ''}" 
        FAQ_TEXTS.append(normalize(content))
    
    # BOOKS
    cur.execute(
        """
        SELECT b.name, b.author, b.year, b.quantity, b.status, m.name
        FROM books b LEFT JOIN majors m ON b.major_id = m.major_id
    """
    )
    book_rows = cur.fetchall()
    BOOK_TEXTS = [
        normalize(f"s√°ch {n}. t√°c gi·∫£ {a}. ng√†nh {m or ''}")
        for n, a, _, _, _, m in book_rows
    ]

    # MAJORS
    cur.execute("SELECT name, major_id, description FROM majors")
    major_rows = cur.fetchall()
    MAJOR_TEXTS = [
        normalize(f"ng√†nh {n}. m√£ {mid}. {desc or ''}")
        for n, mid, desc in major_rows
    ]

    conn.close()

    print("ƒêang t·∫°o embedding (l·∫ßn ƒë·∫ßu s·∫Ω h∆°i l√¢u)...")
    FAQ_EMB = (
        embed_model.encode(FAQ_TEXTS, normalize_embeddings=True)
        if FAQ_TEXTS
        else np.zeros((0, 768))
    )
    BOOK_EMB = (
        embed_model.encode(BOOK_TEXTS, normalize_embeddings=True)
        if BOOK_TEXTS
        else np.zeros((0, 768))
    )
    MAJOR_EMB = (
        embed_model.encode(MAJOR_TEXTS, normalize_embeddings=True)
        if MAJOR_TEXTS
        else np.zeros((0, 768))
    )

    print(f"‚úÖ ƒê√£ t·∫£i: FAQ={len(faq_rows)} | BOOKS={len(book_rows)} | MAJORS={len(major_rows)}")


# ============================================
#  ROUTER ‚Äì FALLBACK B·∫∞NG EMBEDDING
# ============================================
def auto_route_by_embedding(q_vec: np.ndarray) -> str:
    """
    N·∫øu LLM ph√¢n lo·∫°i linh tinh ‚Üí d√πng embedding ch·ªçn b·∫£ng n√†o g·∫ßn nh·∫•t.
    """
    best_type = "FAQ"
    best_score = -1.0

    if len(FAQ_EMB) > 0:
        s = float(np.max(np.dot(FAQ_EMB, q_vec)))
        best_type, best_score = "FAQ", s

    if len(BOOK_EMB) > 0:
        s = float(np.max(np.dot(BOOK_EMB, q_vec)))
        if s > best_score:
            best_type, best_score = "BOOKS", s

    if len(MAJOR_EMB) > 0:
        s = float(np.max(np.dot(MAJOR_EMB, q_vec)))
        if s > best_score:
            best_type, best_score = "MAJORS", s

    return best_type


# ============================================
#  LOAD TRAINED MODEL (ML Classification)
# ============================================
import torch
from model import NeuralNet
from nltk_utils import bag_of_words, tokenize

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

FILE = "data.pth"
try:
    data = torch.load(FILE, map_location=device)
    input_size = data["input_size"]
    hidden_size = data["hidden_size"]
    output_size = data["output_size"]
    all_words = data["all_words"]
    tags = data["tags"]
    model_state = data["model_state"]

    model = NeuralNet(input_size, hidden_size, output_size).to(device)
    model.load_state_dict(model_state)
    model.eval()
    print("‚úÖ ƒê√£ load model ph√¢n lo·∫°i (data.pth)")
except Exception as e:
    print(f"‚ö† Kh√¥ng load ƒë∆∞·ª£c model ph√¢n lo·∫°i: {e}")
    model = None

def predict_intent(sentence):
    if not model:
        return None
    
    sentence = tokenize(sentence)
    X = bag_of_words(sentence, all_words)
    X = X.reshape(1, X.shape[0])
    X = torch.from_numpy(X).to(device)

    output = model(X)
    _, predicted = torch.max(output, dim=1)
    tag = tags[predicted.item()]

    probs = torch.softmax(output, dim=1)
    prob = probs[0][predicted.item()]
    
    if prob.item() > 0.75:
        return tag
    return None

# ============================================
# 1) ROUTER ‚Äì HYBRID (ML + LLM)
# ============================================
def route_llm(question: str, q_vec: np.ndarray) -> str:
    # B1: D√πng model ƒë√£ train ƒë·ªÉ ph√¢n lo·∫°i Category (ML c∆° b·∫£n)
    # Model n√†y ƒë√£ ƒë∆∞·ª£c train tr√™n C√ÇU TR·∫¢ L·ªúI t·ª´ Notion
    intent = predict_intent(question)
    
    if intent:
        print(f"[ML Predict] Intent: {intent}")
        # N·∫øu l√† GREETING -> Tr·∫£ v·ªÅ lu√¥n
        if intent == "GREETING":
            return "GREETING"
        
        # N·∫øu ra c√°c Category c·ª• th·ªÉ (Gi·ªù m·ªü c·ª≠a, Li√™n h·ªá...) -> Tr·∫£ v·ªÅ ch√≠nh Category ƒë√≥
        # ƒê·ªÉ l√°t n·ªØa search_faq ch·ªâ t√¨m trong category n√†y th√¥i.
        return intent

    # B2: N·∫øu model kh√¥ng ch·∫Øc ch·∫Øn (ho·∫∑c l√† c√¢u h·ªèi v·ªÅ S√°ch/Ng√†nh m√† model ch∆∞a h·ªçc k·ªπ)
    # D√πng LLM ƒë·ªÉ ph√¢n lo·∫°i chung
    prompt = f"""
Ph√¢n lo·∫°i c√¢u h·ªèi c·ªßa sinh vi√™n v√†o 1 trong 3 nh√≥m sau:

1. FAQ: H·ªèi v·ªÅ quy ƒë·ªãnh, th·ªß t·ª•c, gi·ªù m·ªü c·ª≠a, li√™n h·ªá, m∆∞·ª£n tr·∫£ s√°ch, wifi, t√†i kho·∫£n...
2. BOOKS: H·ªèi t√¨m s√°ch, gi√°o tr√¨nh, t√†i li·ªáu, t√°c gi·∫£, ki·ªÉm tra s√°ch c√≤n kh√¥ng...
3. MAJORS: H·ªèi th√¥ng tin v·ªÅ c√°c ng√†nh h·ªçc, m√£ ng√†nh, ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o...

C√¢u h·ªèi: "{question}"

Ch·ªâ tr·∫£ v·ªÅ ƒë√∫ng 1 t·ª´: FAQ ho·∫∑c BOOKS ho·∫∑c MAJORS.
"""
    out = llm(prompt, temp=0.05, n=10).upper().strip()

    if out in ["FAQ", "BOOKS", "MAJORS"]:
        return out

    # fallback embedding
    return auto_route_by_embedding(q_vec)
# ============================================
# 2) REWRITE ‚Äì KH√îNG ƒê·ª§NG C√ÇU NG·∫ÆN
# ============================================
def rewrite_question(q: str) -> str:
    # C√¢u ng·∫Øn (‚â§ 5 t·ª´) ‚Üí gi·ªØ nguy√™n, tr√°nh LLM ph√° nghƒ©a.
    # UPDATE: V·ªõi y√™u c·∫ßu "hi·ªÉu nh∆∞ ng∆∞·ªùi", ta cho LLM s·ª≠a c·∫£ c√¢u ng·∫Øn n·∫øu n√≥ qu√° t·ªëi nghƒ©a.
    # Ch·ªâ b·ªè qua n·∫øu qu√° ng·∫Øn (< 2 t·ª´)
    if len(q.split()) < 2:
        return q

    prompt = f"""
B·∫°n l√† m·ªôt tr·ª£ l√Ω th√¥ng minh. H√£y ƒê·ªåC HI·ªÇU √Ω ƒë·ªãnh c·ªßa ng∆∞·ªùi d√πng v√† vi·∫øt l·∫°i c√¢u h·ªèi sao cho r√µ r√†ng, ƒë·∫ßy ƒë·ªß nghƒ©a nh·∫•t.
N·∫øu c√¢u h·ªèi qu√° ng·∫Øn, d√πng t·ª´ ƒëa nghƒ©a ho·∫∑c thi·∫øu ch·ªß ng·ªØ, h√£y di·ªÖn gi·∫£i l·∫°i theo c√°ch ng∆∞·ªùi b√¨nh th∆∞·ªùng s·∫Ω h·ªèi ƒë·∫ßy ƒë·ªß.
ƒê·∫∂C BI·ªÜT:
- N·∫øu h·ªèi v·ªÅ "s·ªë", "g·ªçi", "alo" -> Th√™m t·ª´ kh√≥a "s·ªë ƒëi·ªán tho·∫°i li√™n h·ªá hotline".
- N·∫øu h·ªèi v·ªÅ "·ªü ƒë√¢u", "ch·ªó n√†o" -> Th√™m t·ª´ kh√≥a "ƒë·ªãa ƒëi·ªÉm v·ªã tr√≠".

V√≠ d·ª•:
- "s·ªë n√†o" -> "s·ªë ƒëi·ªán tho·∫°i li√™n h·ªá hotline l√† g√¨"
- "m·ªü c·ª≠a ko" -> "gi·ªù m·ªü c·ª≠a ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o"
- "li√™n h·ªá sao" -> "c√°ch th·ª©c li√™n h·ªá v·ªõi th∆∞ vi·ªán"

C√¢u g·ªëc: "{q}"

C√¢u vi·∫øt l·∫°i (ch·ªâ vi·∫øt 1 c√¢u duy nh·∫•t):
"""
    out = llm(prompt, temp=0.1, n=64)
    return out.strip() if out else q


# ============================================
# 3A) SEMANTIC SEARCH CHO FAQ ‚Äì C√ì L·ªåC CATEGORY
# ============================================
def search_faq_candidates(q_vec: np.ndarray, top_k: int = 10, filter_category: str = None): 
    if len(FAQ_EMB) == 0:
        return []

    sims = np.dot(FAQ_EMB, q_vec)
    idx = np.argsort(-sims)[:top_k]

    candidates = []
    for i in idx:
        score = float(sims[i])
        # H·∫° ng∆∞·ª°ng xu·ªëng c·ª±c th·∫•p ƒë·ªÉ "l∆∞·ªõi" ƒë∆∞·ª£c h·∫øt c√°c c√¢u c√≥ √Ω nghƒ©a li√™n quan
        if score < 0.08: 
            continue
        
        q, a, cat = faq_rows[i]
        
        # L·ªåC: N·∫øu ƒë√£ bi·∫øt Category (do model train d·ª± ƒëo√°n), ch·ªâ l·∫•y ƒë√∫ng Category ƒë√≥
        if filter_category and filter_category not in ["FAQ", "BOOKS", "MAJORS", "GREETING"]:
            # So s√°nh t∆∞∆°ng ƒë·ªëi (v√¨ c√≥ th·ªÉ c√≥ s·ª± kh√°c bi·ªát nh·ªè v·ªÅ string)
            if cat != filter_category:
                continue

        candidates.append(
            {
                "score": score,
                "question": q or "",
                "answer": a or "",
                "category": cat or "",
                "id": i,
            }
        )
    return candidates


# ============================================
# 3B) SEMANTIC SEARCH CHO BOOKS / MAJORS
# ============================================
def search_nonfaq(table: str, q_vec: np.ndarray, top_k: int = 10):
    candidates = []

    if table == "BOOKS":
        if len(BOOK_EMB) == 0:
            return []
        sims = np.dot(BOOK_EMB, q_vec)
        rows = book_rows
        th = 0.15 # H·∫° ng∆∞·ª°ng th·∫•p ƒë·ªÉ Rerank l·ªçc
        idx = np.argsort(-sims)[:top_k]
        for i in idx:
            score = float(sims[i])
            if score < th:
                continue
            n, a, y, qty, s, m = rows[i]
            # Format n·ªôi dung ƒë·ªÉ LLM ƒë·ªçc hi·ªÉu
            content = f"S√°ch: {n}. T√°c gi·∫£: {a}. NƒÉm: {y}. S·ªë l∆∞·ª£ng: {qty}. T√¨nh tr·∫°ng: {s}. Ng√†nh: {m or 'Chung'}"
            candidates.append({
                "score": score,
                "question": "", 
                "answer": content,
                "category": "BOOKS",
                "id": i
            })
        return candidates

    # MAJORS
    if len(MAJOR_EMB) == 0:
        return []
    sims = np.dot(MAJOR_EMB, q_vec)
    rows = major_rows
    th = 0.20 
    idx = np.argsort(-sims)[:top_k]
    for i in idx:
        score = float(sims[i])
        if score < th:
            continue
        name, code, desc = rows[i]
        content = f"Ng√†nh: {name}. M√£ ng√†nh: {code}. M√¥ t·∫£: {desc or 'ƒêang c·∫≠p nh·∫≠t'}"
        candidates.append({
            "score": score,
            "question": "",
            "answer": content,
            "category": "MAJORS",
            "id": i
        })
    return candidates


# ============================================
# 3C) LLM RERANK CHO FAQ ‚Äì CH·ªñ ‚ÄúHI·ªÇU NGHƒ®A‚Äù
# ============================================
def rerank_with_llm(user_q: str, candidates: list):
    if not candidates:
        return None

    # 1. T·∫°o block text cho LLM ƒë·ªçc
    block = ""
    for i, c in enumerate(candidates, start=1):
        block += f"{i}. [{c['category']}] {c['answer']}\n"

    # 2. Prompt "T∆∞ duy" (Reasoning) thay v√¨ "Lu·∫≠t" (Rules)
    prompt = f"""
B·∫°n l√† chuy√™n gia t∆∞ v·∫•n th√¥ng minh.
Nhi·ªám v·ª•: T√¨m c√¢u tr·∫£ l·ªùi PH√ô H·ª¢P NH·∫§T cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng trong danh s√°ch b√™n d∆∞·ªõi.

C√¢u h·ªèi: "{user_q}"

Danh s√°ch ·ª©ng vi√™n:
{block}

H∆Ø·ªöNG D·∫™N T∆Ø DUY:
- H√£y hi·ªÉu √ù NGHƒ®A c·ªßa c√¢u h·ªèi (kh√¥ng ch·ªâ b·∫Øt t·ª´ kh√≥a).
- V√≠ d·ª•: H·ªèi "Fanpage" th√¨ c√¢u ch·ª©a "Facebook" l√† ƒë√∫ng. H·ªèi "Quy tr√¨nh" th√¨ c√¢u h∆∞·ªõng d·∫´n c√°c b∆∞·ªõc l√† ƒë√∫ng.
- N·∫øu c√¢u h·ªèi t√¨m "ƒê·ªãa ƒëi·ªÉm" (·ªü ƒë√¢u), h√£y ch·ªçn c√¢u ch·ª©a th√¥ng tin v·ªã tr√≠.
- N·∫øu c√¢u h·ªèi t√¨m "Danh s√°ch" (g·ªìm nh·ªØng g√¨), h√£y ch·ªçn c√¢u li·ªát k√™ ƒë·∫ßy ƒë·ªß nh·∫•t.

Y√äU C·∫¶U:
- N·∫øu t√¨m th·∫•y c√¢u tr·∫£ l·ªùi ph√π h·ª£p: Tr·∫£ v·ªÅ S·ªê TH·ª® T·ª∞ (v√≠ d·ª•: 1, 2...).
- N·∫øu kh√¥ng c√≥ c√¢u n√†o kh·ªõp: Tr·∫£ v·ªÅ 0.

Ch·ªâ tr·∫£ v·ªÅ 1 con s·ªë duy nh·∫•t.
"""
    # TƒÉng n l√™n 128 ƒë·ªÉ tr√°nh b·ªã c·∫Øt gi·ªØa ch·ª´ng
    out = llm(prompt, temp=0.1, n=128).strip()
    
    # 3. Parse k·∫øt qu·∫£
    import re
    match = re.search(r'\d+', out)
    if match:
        idx = int(match.group()) - 1
        # N·∫øu LLM ch·ªçn 0 ho·∫∑c s·ªë kh√¥ng h·ª£p l·ªá -> Coi nh∆∞ kh√¥ng ch·ªçn ƒë∆∞·ª£c
        if 0 <= idx < len(candidates):
            return candidates[idx]
            
    # 4. FALLBACK TH√îNG MINH (Quan tr·ªçng!)
    # N·∫øu LLM kh√¥ng ch·ªçn ƒë∆∞·ª£c (tr·∫£ v·ªÅ 0 ho·∫∑c l·ªói), nh∆∞ng Search Engine (Embedding) 
    # ƒë√£ t√¨m ra ·ª©ng vi√™n s·ªë 1 c√≥ ƒëi·ªÉm s·ªë r·∫•t cao (> 0.45), th√¨ tin t∆∞·ªüng Search Engine.
    # (V√¨ Embedding model BAAI/bge-m3 r·∫•t m·∫°nh, th∆∞·ªùng top 1 l√† ƒë√∫ng)
    if candidates and candidates[0]['score'] > 0.45:
        print(f"[Rerank] LLM t·ª´ ch·ªëi, nh∆∞ng Top 1 score cao ({candidates[0]['score']:.2f}) -> Ch·ªçn Top 1.")
        return candidates[0]

    return None


def strict_answer(question: str, knowledge: str) -> str:
    print(f"[DEBUG STRICT] Q: {question} | Knowledge: {knowledge[:50]}...")
    prompt = f"""
B·∫°n l√† tr·ª£ l√Ω ·∫£o c·ªßa th∆∞ vi·ªán. 
NHI·ªÜM V·ª§: Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin cung c·∫•p b√™n d∆∞·ªõi.

TH√îNG TIN (KNOWLEDGE):
{knowledge}

C√ÇU H·ªéI (QUESTION): "{question}"

QUY T·∫ÆC B·∫ÆT BU·ªòC:
1. ‚ö†Ô∏è TUY·ªÜT ƒê·ªêI TR·∫¢ L·ªúI B·∫∞NG TI·∫æNG VI·ªÜT.
2. N·∫øu th√¥ng tin c√≥ v·∫ª li√™n quan (d√π ch·ªâ m·ªôt ph·∫ßn), H√ÉY TR·∫¢ L·ªúI NGAY.
3. V√≠ d·ª•: H·ªèi "s√°ch c√¥ng ngh·ªá" m√† c√≥ "C√¥ng ngh·ªá ph·∫ßn m·ªÅm" -> TR·∫¢ L·ªúI th√¥ng tin s√°ch ƒë√≥.
4. N·∫øu th√¥ng tin l√† danh s√°ch, h√£y tr√≠ch xu·∫•t √Ω ch√≠nh.
5. ‚ö†Ô∏è ƒê·ªêI V·ªöI T√äN RI√äNG (T√°c gi·∫£, T√™n s√°ch, Ng∆∞·ªùi li√™n h·ªá...): PH·∫¢I TR√çCH XU·∫§T CH√çNH X√ÅC 100%, KH√îNG ƒê∆Ø·ª¢C R√öT G·ªåN (V√≠ d·ª•: "Nguy·ªÖn Th·ªã A" kh√¥ng ƒë∆∞·ª£c vi·∫øt l√† "Nguy·ªÖn Th·ªã").
6. N·∫øu c√¢u h·ªèi d√πng t·ª´ ƒë·ªìng nghƒ©a, h√£y t·ª± suy lu·∫≠n.
7. N·∫øu c√≥ s·ªë li·ªáu/th·ªëng k√™, h√£y ƒë∆∞a ra con s·ªë ƒë√≥.
8. Tuy·ªát ƒë·ªëi KH√îNG tr·∫£ l·ªùi "{FALLBACK_MSG}" n·∫øu b·∫°n t√¨m th·∫•y th√¥ng tin li√™n quan.

N·∫øu th√¥ng tin HO√ÄN TO√ÄN KH√îNG LI√äN QUAN th√¨ m·ªõi n√≥i: "{FALLBACK_MSG}"

C√¢u tr·∫£ l·ªùi c·ªßa b·∫°n (Ti·∫øng Vi·ªát):
"""
    # TƒÉng temp l√™n ƒë·ªÉ bot "d√°m" tr·∫£ l·ªùi h∆°n -> UPDATE: Gi·∫£m xu·ªëng ƒë·ªÉ ch√≠nh x√°c t√™n ri√™ng
    out = llm(prompt, temp=0.05, n=256) 
    print(f"[DEBUG STRICT OUT] {out}")
    
    if not out:
        return FALLBACK_MSG

    out = out.strip()
    
    # UPDATE: Ch·∫•p nh·∫≠n SƒêT (s·ªë) ho·∫∑c Email (@) ho·∫∑c Link (http)
    if any(c.isdigit() for c in out) or "@" in out or "http" in out:
        return out

    # B·ªè check "kh√¥ng c√≥ th√¥ng tin" qu√° g·∫Øt, ch·ªâ check n·∫øu output qu√° ng·∫Øn
    if "kh√¥ng c√≥ th√¥ng tin" in out.lower() and len(out) < 15: 
         return FALLBACK_MSG

    return out


# ============================================
#  MAIN PROCESS
# ============================================
def process_message(text: str) -> str:
    print("[CHAT.PY] ƒê√É G·ªåI N√ÉO")
    if not text.strip():
        return "Xin ch√†o üëã B·∫°n mu·ªën h·ªèi th√¥ng tin g√¨ trong th∆∞ vi·ªán?"

    # B0: vector cho router
    q_vec_route = embed_model.encode(normalize(text), normalize_embeddings=True)

    # B1: Router
    route = route_llm(text, q_vec_route)
    # print("[DEBUG ROUTE]", route)

    # B2: Rewrite
    rewritten = rewrite_question(text)
    q_vec = embed_model.encode(normalize(rewritten), normalize_embeddings=True)

    # B3 + B4
    # UPDATE: N·∫øu c√¢u h·ªèi d√†i (> 3 t·ª´) ho·∫∑c ch·ª©a t·ª´ kh√≥a h·ªèi (·ªü ƒë√¢u, s√°ch, ph√≤ng, bao nhi√™u...), 
    # th√¨ D√ô Router b·∫£o l√† GREETING c≈©ng K·ªÜ N√ì, c·ª© ƒëi t√¨m ki·∫øm cho ch·∫Øc.
    # Tr√°nh tr∆∞·ªùng h·ª£p model train b·ªã l·ªách, c·ª© th·∫•y l·∫° l√† ph√°n Greeting.
    is_real_question = len(text.split()) > 3 or any(w in text.lower() for w in ["·ªü ƒë√¢u", "s√°ch", "ph√≤ng", "bao nhi√™u", "khi n√†o", "m·∫•y gi·ªù", "l√† g√¨"])
    
    if route == "GREETING" and not is_real_question:
        return "Xin ch√†o! T√¥i l√† tr·ª£ l√Ω ·∫£o th∆∞ vi·ªán (ƒë√£ ƒë∆∞·ª£c train). B·∫°n c·∫ßn t√¨m s√°ch, h·ªèi quy ƒë·ªãnh hay th√¥ng tin ng√†nh h·ªçc?"

    # HEURISTIC: S·ª≠a l·ªói Router ƒëo√°n sai (v√≠ d·ª•: "s√°ch python" -> GREETING)
    lower_text = text.lower()
    
    # 1. N·∫øu c√≥ t·ª´ kh√≥a S√ÅCH/GI√ÅO TR√åNH m√† kh√¥ng c√≥ t·ª´ kh√≥a QUY TR√åNH -> Force BOOKS
    if any(w in lower_text for w in ["s√°ch", "gi√°o tr√¨nh", "t√†i li·ªáu", "t√°c gi·∫£", "·∫•n ph·∫©m"]):
        # Tr·ª´ c√°c tr∆∞·ªùng h·ª£p h·ªèi quy ƒë·ªãnh (m∆∞·ª£n, tr·∫£, ph√≤ng, gi·ªù...)
        if not any(w in lower_text for w in ["m∆∞·ª£n", "tr·∫£", "quy ƒë·ªãnh", "n·ªôi quy", "gi·ªù", "ph√≤ng", "th·ªß t·ª•c", "h∆∞·ªõng d·∫´n"]):
            print("[DEBUG] Heuristic: Force route -> BOOKS")
            route = "BOOKS"

    # 2. N·∫øu c√≥ t·ª´ kh√≥a NG√ÄNH/KHOA -> Force MAJORS
    if any(w in lower_text for w in ["ng√†nh", "khoa", "ƒë√†o t·∫°o", "m√£ ng√†nh"]):
        print("[DEBUG] Heuristic: Force route -> MAJORS")
        route = "MAJORS"

    # N·∫øu route l√† BOOKS ho·∫∑c MAJORS -> X·ª≠ l√Ω ri√™ng
    if route == "BOOKS":
        candidates = search_nonfaq("BOOKS", q_vec, top_k=15)
        if not candidates:
             return "Kh√¥ng t√¨m th·∫•y s√°ch n√†o ph√π h·ª£p."
        
        print(f"[DEBUG BOOKS] Found {len(candidates)} candidates.")
        best_cand = rerank_with_llm(rewritten, candidates)
        if not best_cand:
             # Fallback top 1
             best_cand = candidates[0]
        
        return strict_answer(rewritten, best_cand['answer'])

    if route == "MAJORS":
        candidates = search_nonfaq("MAJORS", q_vec, top_k=15)
        if not candidates:
             return "Kh√¥ng t√¨m th·∫•y ng√†nh h·ªçc n√†o ph√π h·ª£p."
        
        print(f"[DEBUG MAJORS] Found {len(candidates)} candidates.")
        best_cand = rerank_with_llm(rewritten, candidates)
        if not best_cand:
             best_cand = candidates[0]
             
        return strict_answer(rewritten, best_cand['answer'])
    
    # Tr∆∞·ªùng h·ª£p c√≤n l·∫°i: FAQ ho·∫∑c C√ÅC CATEGORY C·ª§ TH·ªÇ
    # N·∫øu route kh√¥ng ph·∫£i l√† "FAQ" chung chung, th√¨ n√≥ ch√≠nh l√† filter_category
    filter_cat = route if route != "FAQ" else None
    
    print(f"\n[DEBUG] Filter Category: {filter_cat}")

    # B∆Ø·ªöC 1: T√¨m TO√ÄN B·ªò FAQ (B·ªè l·ªçc Category ƒë·ªÉ tƒÉng Recall)
    candidates = search_faq_candidates(q_vec, top_k=20, filter_category=None)
        
    if not candidates:
        print("[DEBUG] ‚ùå Kh√¥ng t√¨m th·∫•y candidate n√†o (do ƒëi·ªÉm th·∫•p h∆°n ng∆∞·ª°ng).")
        return "Xin l·ªói, t√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong c∆° s·ªü d·ªØ li·ªáu."

    print(f"[DEBUG] Found {len(candidates)} candidates:")
    for c in candidates:
        print(f"  - [{c['score']:.4f}] {c['answer'][:50]}... (Cat: {c['category']})")

    # Rerank
    best_cand = rerank_with_llm(rewritten, candidates)
    if not best_cand:
            print("[DEBUG] ‚ùå Rerank LLM t·ª´ ch·ªëi t·∫•t c·∫£ candidates.")
            # Fallback: l·∫•y top 1
            best_cand = candidates[0]
    else:
            print(f"[DEBUG] ‚úÖ Rerank ch·ªçn: {best_cand['answer'][:50]}...")

    # Strict Answer
    final_ans = strict_answer(rewritten, best_cand['answer'])
    return final_ans


# ============================================
#  CLI
# ============================================
if __name__ == "__main__":
    print("ü§ñ Chatbot 4-B∆Ø·ªöC (Router ‚Üí Rewrite ‚Üí Search+Rerank ‚Üí Strict Answer) ƒë√£ s·∫µn s√†ng!")
    while True:
        q = input("\nB·∫°n: ")
        if q.lower() in ["quit", "bye", "exit", "tho√°t"]:
            print("H·∫πn g·∫∑p l·∫°i b·∫°n ·ªü th∆∞ vi·ªán nh√©! üìö")
            break
        print("Bot:", process_message(q))
