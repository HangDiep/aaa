import os, random, json, sqlite3, re, time
os.environ["TRANSFORMERS_NO_TF"] = "1"
# chat_fixed.py
import numpy as np
import torch, requests
from model import NeuralNet
from nltk_utils import tokenize, bag_of_words
from state_manager import StateManager
import threading
from dotenv import load_dotenv
from notion_client import Client
from typing import Optional, List, Dict
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import socket
from datetime import datetime
import rapidfuzz
from rapidfuzz import fuzz
from sentence_transformers import SentenceTransformer
import os
import logging

# ·∫®n b·ªõt log c·ªßa TensorFlow
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"  # 0=full, 1=warning+, 2=error+, 3=fatal


# T·∫Øt progress bar c·ªßa HuggingFace Hub (t·∫£i model)
os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"

# Gi·∫£m log c·ªßa transformers & sentence-transformers
logging.getLogger("transformers").setLevel(logging.ERROR)
logging.getLogger("sentence_transformers").setLevel(logging.ERROR)
logging.getLogger("urllib3").setLevel(logging.ERROR)


# ============== C·∫§U H√åNH ==============
ENV_PATH = r"D:\HTML\a\rag\.env"
try:
    if os.path.exists(ENV_PATH):
        load_dotenv(ENV_PATH, override=True)
        # Sau load_dotenv:
except Exception:
    pass

print("=== DEBUG ENV CHECK ===")
print("ENV_PATH =", ENV_PATH, "| exists:", os.path.exists(ENV_PATH))
print("NOTION_API_KEY =", os.getenv("NOTION_API_KEY"))
print("NOTION_BASE_URL =", os.getenv("NOTION_BASE_URL"))
print("DATABASE_ID_FAQ =", os.getenv("DATABASE_ID_FAQ"))
print("========================")

_notion_cached = None
_notion_warned_once = False  # ch·ªâ c·∫£nh b√°o 1 l·∫ßn khi l·ªói HTTP push

# Ollama (c√≥ th·ªÉ t·∫Øt n·∫øu l·ªói m·∫°ng)
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://127.0.0.1:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "qwen2:1.5b")
OLLAMA_TIMEOUT = float(os.getenv("OLLAMA_TIMEOUT", "60"))
ENABLE_OLLAMA_APPEND = os.getenv("ENABLE_OLLAMA_APPEND", "true").lower() != "false"
MAX_OLLAMA_APPEND_TOKENS = 150
print("[Ollama] URL:", OLLAMA_URL, "| model:", OLLAMA_MODEL, "| timeout:", OLLAMA_TIMEOUT)
FAQ_API_URL = "http://localhost:8000/search"
INVENTORY_API_URL = "http://localhost:8000/inventory"

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CHAT_DB_PATH = os.path.join(BASE_DIR, "chat.db")
print(f"[ChatDB] Using: {CHAT_DB_PATH}")
DB_PATH = CHAT_DB_PATH

FAQ_DB_PATH = os.path.join(BASE_DIR, "faq.db")
CONF_THRESHOLD = 0.60
LOG_ALL_QUESTIONS = True

INTERRUPT_INTENTS = set()
CANCEL_WORDS = {"h·ªßy", "hu·ª∑", "huy", "cancel", "tho√°t", "d·ª´ng", "ƒë·ªïi ch·ªß ƒë·ªÅ", "doi chu de"}
LAST_BOOK_CONTEXT = None

import unicodedata

def normalize_vi(text: str) -> str:
    text = (text or "").lower().strip()
    text = unicodedata.normalize("NFD", text)
    text = "".join(ch for ch in text if unicodedata.category(ch) != "Mn")
    return re.sub(r"\s+", " ", text)
# ========== EMBEDDING MODEL ==========
try:
    embed_model = SentenceTransformer("keepitreal/vietnamese-sbert")
except Exception:
    embed_model = None  # fallback

# ============== DB helpers ==============
def ensure_main_db() -> sqlite3.Connection:
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True) if os.path.dirname(DB_PATH) else None
    conn = sqlite3.connect(DB_PATH, check_same_thread=False)
    cur = conn.cursor()
    cur.execute(
        """
        CREATE TABLE IF NOT EXISTS conversations (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_message TEXT,
            bot_reply   TEXT,
            intent_tag  TEXT,
            confidence  REAL,
            time        TEXT
        );
        """
    )
    conn.commit()
    return conn
def _now():
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
def ensure_questions_log_db() -> None:
    dir_name = os.path.dirname(FAQ_DB_PATH)
    if dir_name and not os.path.exists(dir_name):
        os.makedirs(dir_name, exist_ok=True)
    conn2 = sqlite3.connect(FAQ_DB_PATH)
    cur2 = conn2.cursor()
    cur2.execute(
        """
        CREATE TABLE IF NOT EXISTS questions_log (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            question   TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            synced     INTEGER DEFAULT 0
        )
        """
    )
    conn2.commit()
    conn2.close()

def log_question_for_notion(question: str) -> None:
    if not question or not question.strip():
        return
    ensure_questions_log_db()
    conn2 = sqlite3.connect(FAQ_DB_PATH)
    cur2 = conn2.cursor()
    cur2.execute(
        "INSERT INTO questions_log (question, synced) VALUES (?, 0)",
        (question.strip(),),
    )
    conn2.commit()
    conn2.close()

# ============== Model load ==============
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

with open("intents.json", "r", encoding="utf-8-sig") as f:
    intents = json.load(f)

_data = torch.load("data.pth", map_location=device)
input_size  = _data["input_size"]
hidden_size = _data["hidden_size"]
output_size = _data["output_size"]
all_words   = _data["all_words"]
tags        = _data["tags"]
model_state = _data["model_state"]

model = NeuralNet(input_size, hidden_size, output_size).to(device)
model.load_state_dict(model_state)
model.eval()

try:
    state_mgr = StateManager("flows.json")
except Exception:
    state_mgr = StateManager()




# ============== FAQ / Inventory ==============
def get_faq_response(sentence: str) -> Optional[str]:
    try:
        resp = requests.get(FAQ_API_URL, params={"q": sentence}, timeout=5)
        if resp.status_code != 200:
            print(f"[FAQ] HTTP {resp.status_code}: {resp.text[:200]}")
            return None
        data = resp.json()
        if not isinstance(data, list) or not data:
            return None
        lines: List[str] = []
        lines.append("üìñ **K·∫øt qu·∫£ FAQ:**\n")
        lines.append("| C√¢u h·ªèi | Tr·∫£ l·ªùi |")
        lines.append("|---------|---------|")
        for item in data:
            q = item.get("question", "").strip()
            a = item.get("answer", "").strip()
            if q or a:
                q = q.replace("|", "ÔΩú")
                a = a.replace("|", "ÔΩú")
                lines.append(f"| {q} | {a} |")
        return "\n".join(lines) if len(lines) > 3 else None
    except requests.RequestException as e:
        print(f"[FAQ] L·ªói k·∫øt n·ªëi API: {e}")
        return None
    except Exception as e:
        print(f"[FAQ] L·ªói x·ª≠ l√Ω d·ªØ li·ªáu: {e}")
        return None

def get_inventory_response(sentence: str) -> Optional[str]:
    try:
        resp = requests.get(INVENTORY_API_URL, params={"book_name": sentence}, timeout=5)
        if resp.status_code != 200:
            print(f"[Inventory] HTTP {resp.status_code}: {resp.text[:200]}")
            return None
        data = resp.json()
        if isinstance(data, list) and data:
            book = data[0]
            name = book.get("name")
            author = book.get("author", "?")
            year = book.get("year", "?")
            quantity = book.get("quantity", "?")
            status = book.get("status", "?")
            if name:
                return (
                    f"S√°ch '{name}' c·ªßa t√°c gi·∫£ {author}, nƒÉm xu·∫•t b·∫£n {year}, "
                    f"s·ªë l∆∞·ª£ng: {quantity}, tr·∫°ng th√°i: {status}"
                )
        return None
    except requests.RequestException as e:
        print(f"[Inventory] L·ªói k·∫øt n·ªëi API: {e}")
        return None
    except Exception as e:
        print(f"[Inventory] L·ªói x·ª≠ l√Ω d·ªØ li·ªáu: {e}")
        return None



# ============== CORE chat ==============

def ollama_alive() -> bool:
    try:
        r = requests.get(f"{OLLAMA_URL.rstrip('/')}/api/tags", timeout=3)
        return r.status_code == 200
    except Exception:
        return False
        


def _dns_ok(host: str, timeout_s: float = 3.0) -> bool:
    try:
        socket.setdefaulttimeout(timeout_s)
        socket.getaddrinfo(host, 443)
        return True
    except Exception:
        return False
def pull_approved_from_notion_to_sqlite():
    token, dbid, mode, base = _resolve_notion_env()
    url = f"{base.rstrip('/')}/databases/{dbid}/query"
    headers = {
        "Authorization": f"Bearer {token}",
        "Notion-Version": os.getenv("NOTION_VERSION", "2022-06-28"),
        "Content-Type": "application/json",
    }
    body = {
        "filter": {
            "and": [
                {"property": "Approved", "checkbox": {"equals": True}},
                # N·∫øu b·∫°n t·∫°o th√™m c·ªôt "Synced" (checkbox) trong Notion:
                # {"property": "Synced", "checkbox": {"equals": False}},
            ]
        }
    }
    r = requests.post(url, headers=headers, json=body, timeout=12)
    r.raise_for_status()
    data = r.json()

    conn = ensure_main_db()
    cur = conn.cursor()

    for row in data.get("results", []):
        props = row.get("properties", {})
        q = props.get("Question", {}).get("rich_text", [{}])[0].get("plain_text", "")
        a = props.get("Answer", {}).get("rich_text", [{}])[0].get("plain_text", "")

        # l∆∞u v√†o SQLite (v√≠ d·ª• conversations hay b·∫£ng ri√™ng)
        cur.execute(
            "INSERT INTO conversations(user_message, bot_reply, intent_tag, confidence, time) VALUES (?,?,?,?,?)",
            (q, a, None, 1.0, _now()),
        )
        # ƒê√°nh d·∫•u ƒë√£ sync n·∫øu b·∫°n c√≥ c·ªôt Synced trong Notion:
        # page_id = row["id"]
        # requests.patch(f"{base.rstrip('/')}/pages/{page_id}",
        #    headers={**headers, "Content-Type": "application/json"},
        #    json={"properties": {"Synced": {"checkbox": True}}})

    conn.commit()
    conn.close()

# ============== Notion helpers (ntn_ token, auto-mapping) ==============
from functools import lru_cache

def _resolve_notion_env():
    try:
        if os.path.exists(ENV_PATH):
            load_dotenv(ENV_PATH, override=True)
    except Exception:
        pass
    token = os.getenv("NOTION_TOKEN") or os.getenv("NOTION_API_KEY") or ""
    dbid  = (
        os.getenv("NOTION_DATABASE_ID")
        or os.getenv("DATABASE_ID_FAQ")
        or os.getenv("DATABASE_ID_BOOKS")
        or os.getenv("DATABASE_ID_MAJORS")
        or ""
    )
    base  = (os.getenv("NOTION_BASE_URL") or "https://api.notion.com/v1").rstrip("/")
    mode  = "sdk" if token.startswith("secret_") else "http"  # ntn_ => http

    # Fallback an to√†n n·∫øu ƒëang tr·ªè t·ªõi ntn-api nh∆∞ng DNS/route h·ªèng
    if token.startswith("ntn_") and "ntn-api.notion.so" in base:
        if not _dns_ok("ntn-api.notion.so"):
            base = "https://api.notion.com/v1"

    return token, dbid, mode, base

def _rt(txt: str):
    return [{"type": "text", "text": {"content": txt or ""}}]

def _http_session_with_retry(total=2, backoff=0.6):
    s = requests.Session()
    retry = Retry(
        total=total,
        backoff_factor=backoff,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET", "POST", "HEAD"],
    )
    s.mount("https://", HTTPAdapter(max_retries=retry))
    s.mount("http://", HTTPAdapter(max_retries=retry))
    return s

def _ntn_session():
    # nh·∫π h∆°n, ∆∞u ti√™n gi·∫£m ch·ªù
    return _http_session_with_retry(total=1, backoff=0.4)

def ntn_ok(base: str) -> bool:
    """Preflight: confirm CF/Notion ph·∫£n h·ªìi ƒë·ªÉ tr√°nh timeout k√©o d√†i."""
    base = (base or "").rstrip("/")
    try:
        r = requests.get("https://api.notion.com/v1/status", timeout=6)
        if r.status_code not in (200, 400, 401, 405):
            print("[Preflight] api.notion.com status:", r.status_code)
    except requests.exceptions.RequestException:
        return False

    if "ntn-api.notion.so" in base:
        try:
            rr = requests.head(f"{base}/pages", timeout=6)
            return rr.status_code in (200,201,400,401,403,405,429,500,502,503,504,530)
        except requests.exceptions.RequestException:
            return False
    return True

def _http_create_page(token: str, base_url: str, payload: dict, timeout_s: float = 15.0):
    """POST /pages, tr·∫£ (ok, status, body_text)."""
    url = f"{base_url.rstrip('/')}/pages"
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json",
        "Accept": "application/json",
        "User-Agent": "Mozilla/5.0",
        "Notion-Version": os.getenv("NOTION_VERSION", "2022-06-28"),
        "Host": "ntn-api.notion.so" if "ntn-api.notion.so" in base_url else "api.notion.com",
    }
    try:
        sess = _ntn_session()
        r = sess.post(url, headers=headers, json=payload, timeout=timeout_s, allow_redirects=True)
        ok = r.status_code in (200, 201)
        return ok, r.status_code, r.text
    except requests.exceptions.Timeout:
        return False, 408, "timeout"
    except Exception as e:
        return False, -1, f"{type(e).__name__}: {e}"

@lru_cache(maxsize=8)
def _fetch_db_schema(token: str, base: str, dbid: str) -> dict:
    """L·∫•y schema DB ƒë·ªÉ auto-map properties (cache theo (token,base,dbid))."""
    url = f"{base.rstrip('/')}/databases/{dbid}"
    headers = {
        "Authorization": f"Bearer {token}",
        "Notion-Version": os.getenv("NOTION_VERSION", "2022-06-28"),
        "Accept": "application/json",
    }
    sess = _http_session_with_retry(total=2, backoff=0.5)
    r = sess.get(url, headers=headers, timeout=10)
    if r.status_code not in (200, 201):
        raise RuntimeError(f"GET /databases/{dbid} FAIL {r.status_code}: {r.text[:500]}")
    return r.json()

def _pick_prop_by_type(props: dict, want_type: str, prefer_names: list[str]) -> Optional[str]:
    """Ch·ªçn t√™n c·ªôt theo type: ∆∞u ti√™n theo danh s√°ch t√™n g·ª£i √Ω, fallback c·ªôt b·∫•t k·ª≥ c√πng type."""
    # ∆∞u ti√™n theo t√™n
    lower_props = {k.lower(): k for k in props.keys()}
    for name in prefer_names:
        key = lower_props.get(name.lower())
        if key and props.get(key, {}).get("type") == want_type:
            return key
    # fallback: l·∫•y c·ªôt ƒë·∫ßu ti√™n c√≥ type ph√π h·ª£p
    for k, v in props.items():
        if v.get("type") == want_type:
            return k
    return None

def _ensure_select_option(token: str, base: str, dbid: str, prop_name: str, option_name: str) -> str:
    """
    ƒê·∫£m b·∫£o option select t·ªìn t·∫°i; n·∫øu ch∆∞a c√≥ s·∫Ω th√™m (best effort).
    Tr·∫£ l·∫°i option_name (c√≥ th·ªÉ ƒë√£ t·ªìn t·∫°i ho·∫∑c v·ª´a t·∫°o).
    """
    # ƒê·ªçc schema
    schema = _fetch_db_schema(token, base, dbid)
    props = schema.get("properties", {})
    prop = props.get(prop_name, {})
    if prop.get("type") != "select":
        return option_name  # kh√¥ng ph·∫£i select th√¨ b·ªè qua

    options = prop.get("select", {}).get("options", []) or []
    names = {opt.get("name"): opt.get("id") for opt in options if isinstance(opt, dict)}
    if option_name in names:
        return option_name

    # Th·ª≠ th√™m option qua update database
    url = f"{base.rstrip('/')}/databases/{dbid}"
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json",
        "Notion-Version": os.getenv("NOTION_VERSION", "2022-06-28"),
    }
    new_opt = {"name": option_name}
    body = {
        "properties": {
            prop_name: {
                "select": {
                    "options": options + [new_opt]
                }
            }
        }
    }
    try:
        r = requests.patch(url, headers=headers, json=body, timeout=12)
        if r.status_code in (200, 201):
            return option_name
        else:
            # Kh√¥ng t·∫°o ƒë∆∞·ª£c option ‚Üí v·∫´n d√πng t√™n option (Notion s·∫Ω reject n·∫øu ch∆∞a c√≥)
            print(f"[Notion] WARN: add select option FAIL {r.status_code}: {r.text[:400]}")
            return option_name
    except Exception as e:
        print(f"[Notion] WARN: add select option error: {e}")
        return option_name


def _build_dynamic_payload_force(dbid: str, q: str, a: str) -> dict:
    title_txt = (q or "C√¢u h·ªèi").strip()[:200]
    today_iso = datetime.now().date().isoformat()

    props = {
        "Question": {"rich_text": [{"type": "text", "text": {"content": q or ""}}]},
        "Answer":   {"rich_text": [{"type": "text", "text": {"content": a or ""}}]},
        # Cho item xu·∫•t hi·ªán ngay ·ªü view ch√≠nh:
        "Approved": {"checkbox": True},  # <-- b·∫≠t n·∫øu view ƒëang l·ªçc Approved = checked
        "Language": {"select": {"name": "Ti·∫øng Vi·ªát"}},  # <-- kh·ªõp filter Language
        "Last Update": {"date": {"start": today_iso}},
    }

    # N·∫øu b·∫£ng c·ªßa b·∫°n B·∫ÆT BU·ªòC c√≥ Category ƒë·ªÉ v√†o view, set th√™m 1 value h·ª£p l·ªá:
    # props["Category"] = {"select": {"name": "Quy ƒë·ªãnh"}}

    return {
        "parent": {"database_id": dbid},
        "properties": props,
    }




def push_to_notion(q: str, a: str):
    """
    ƒê·∫©y ngay t·ª´ng d√≤ng l√™n Notion (ntn_). T·ª± d√≤ schema v√† map properties.
    In l·ªói chi ti·∫øt khi fail ƒë·ªÉ b·∫°n s·ª≠a ƒë√∫ng ch·ªó.
    """
    global _notion_warned_once
    q = (q or "").strip(); a = (a or "").strip()
    if not q:
        return

    token, dbid, mode, base = _resolve_notion_env()
    if not token or not dbid:
        print("[Notion] B·ªè qua: thi·∫øu token/dbid.")
        return

    # Ch·ªâ h·ªó tr·ª£ http (ntn_) ·ªü ƒë√¢y; n·∫øu b·∫°n d√πng secret_, c√≥ th·ªÉ nh√°nh SDK.
    if mode != "http":
        print("[Notion] B·∫°n ƒëang d√πng secret_; nh√°nh HTTP n√†y d√†nh cho ntn_.")
        return

    # Preflight ‚Äì tr√°nh ƒë·ª£i timeout v√¥ √≠ch
    # üëâ Preflight: c√≥ th·ªÉ B·ªé QUA n·∫øu FORCE_PUSH_NOTION=1
    force_push = os.getenv("FORCE_PUSH_NOTION", "0") == "1"
    if not force_push and not ntn_ok(base):
        if not _notion_warned_once:
            print("[Notion] Gateway hi·ªán kh√¥ng reachable ‚Üí b·ªè qua l·∫ßn n√†y.")
            _notion_warned_once = True
        return
    else:
        if force_push:
            print("[Notion] FORCE: b·ªè qua preflight, th·ª≠ push tr·ª±c ti·∫øp...")


    # Build payload theo schema th·ª±c t·∫ø
    try:
        payload = _build_dynamic_payload_force(dbid, q, a)

    except Exception as e:
        print(f"[Notion] Build payload error: {e}")
        return

    ok, status, body = _http_create_page(token, base, payload, timeout_s=15.0)
    if ok:
        print(f"[Notion] OK ({status})")
    else:
        # In body ƒë·∫ßy ƒë·ªß ƒë·ªÉ th·∫•y l·ªói th·∫≠t (property n√†o sai type/t√™n/option)
        print(f"[Notion] FAIL ({status})\n{body[:2000]}")


def _ntn_session():
    s = requests.Session()
    retry = Retry(
        total=1,               # ch·ªâ 1 l·∫ßn retry nh·∫π ƒë·ªÉ kh√¥ng ch·ªù l√¢u
        backoff_factor=0.4,
        status_forcelist=[429,500,502,503,504],
        allowed_methods=["POST", "HEAD"],
    )
    s.mount("https://", HTTPAdapter(max_retries=retry))
    s.mount("http://", HTTPAdapter(max_retries=retry))
    return s
# ============== Ollama append (an to√†n) ==============
def sanitize_vi(extra: str) -> str:
    if not extra: return ""
    extra = re.sub(r'[\u3400-\u9FFF\uF900-\uFAFF]+', '', extra)
    extra = re.sub(r'[\U0001F300-\U0001FAFF]', '', extra)
    extra = extra.replace('‚Äú','').replace('‚Äù','').replace('"','').strip()
    extra = re.sub(r'\s+', ' ', extra)
    banned_starts = ("ch√†o m·ª´ng", "r·∫•t ti·∫øc", "xin ch√†o", "c·∫£m ∆°n")
    if extra.lower().startswith(banned_starts): return ""
    if len(extra.split()) < 3: return ""
    return extra
def get_recent_history(limit=6):
    """L·∫•y lu√¢n phi√™n Q/A g·∫ßn nh·∫•t, m·ªõi ‚Üí c≈© (t·ªëi ƒëa limit d√≤ng)."""
    try:
        conn = sqlite3.connect(DB_PATH)
        cur = conn.cursor()
        cur.execute("""
            SELECT user_message, bot_reply, time
            FROM conversations
            ORDER BY id DESC
            LIMIT ?
        """, (limit,))
        rows = cur.fetchall()
        conn.close()
        # ƒë·∫£o l·∫°i cho th√†nh c≈© ‚Üí m·ªõi
        rows.reverse()
        return rows
    except Exception:
        return []

def ollama_generate_continuation(base_reply: str, user_message: str, max_sentences=3) -> str:
    url = f"{OLLAMA_URL.rstrip('/')}/api/generate"
    history = get_recent_history(limit=8)

    # Gh√©p l·ªãch s·ª≠: Q/A ng·∫Øn g·ªçn
    hist_lines = []
    for q, a, t in history:
        q = (q or "").strip()
        a = (a or "").strip()
        if q or a:
            hist_lines.append(f"- User: {q}")
            hist_lines.append(f"  Bot: {a}")
    hist_block = "\n".join(hist_lines[-14:])  # tr√°nh d√†i qu√°

    system_prompt = (
        "B·∫°n l√† tr·ª£ l√Ω th∆∞ vi·ªán DHTN. D·ª±a v√†o l·ªãch s·ª≠ h·ªôi tho·∫°i d∆∞·ªõi ƒë√¢y, "
        "h√£y VI·∫æT TI·∫æP ph·∫ßn tr·∫£ l·ªùi cho m∆∞·ª£t m√†, ch·ªâ th√™m √Ω b·ªï sung h·ª£p l√Ω, "
        "KH√îNG l·∫∑p l·∫°i nguy√™n vƒÉn, KH√îNG m·ªü ch·ªß ƒë·ªÅ m·ªõi, KH√îNG b·ªãa s·ªë li·ªáu. "
        "N·∫øu l·ªãch s·ª≠ kh√¥ng gi√∫p √≠ch, tr·∫£ v·ªÅ chu·ªói R·ªñNG.\n"
        "Gi·ªõi h·∫°n 1‚Äì3 c√¢u ng·∫Øn. Ch·ªâ ti·∫øng Vi·ªát."
    )

    user_prompt = (
        f"L·ªãch s·ª≠ g·∫ßn ƒë√¢y:\n{hist_block}\n\n"
        f"C√¢u tr·∫£ l·ªùi hi·ªán t·∫°i c·ªßa bot:\n{base_reply}\n\n"
        f"Ng∆∞·ªùi d√πng v·ª´a h·ªèi:\n{user_message}\n\n"
        f"Y√äU C·∫¶U: Vi·∫øt ti·∫øp ng·∫Øn g·ªçn (1‚Äì3 c√¢u) b·ªï sung √Ω d·ª±a tr√™n l·ªãch s·ª≠. "
        f"N·∫øu kh√¥ng ph√π h·ª£p, tr·∫£ v·ªÅ r·ªóng."
    )

    payload = {
        "model": OLLAMA_MODEL,
        "prompt": f"{system_prompt}\n\n{user_prompt}",
        "stream": False,
        "options": {
            "temperature": 0.2,
            "top_p": 0.9,
            "repeat_penalty": 1.15,
            "num_predict": 120
        }
    }

    try:
        r = requests.post(url, json=payload, timeout=OLLAMA_TIMEOUT)
        if r.status_code != 200:
            print(f"[Ollama-continue] HTTP {r.status_code}: {r.text[:200]}")
            return ""
        extra = (r.json().get("response") or "").strip()
        # l√†m s·∫°ch ng·∫Øn g·ªçn
        extra = re.sub(r'\s+', ' ', extra)
        if not extra or extra.lower() in ("", "r·ªóng", "(r·ªóng)"):
            return ""
        # c·∫Øt t·ªëi ƒëa 3 c√¢u
        sentences = [s.strip() for s in re.split(r'[.!?‚Ä¶]+', extra) if s.strip()]
        extra_short = ". ".join(sentences[:max_sentences]).strip()
        return (extra_short + ".") if extra_short and not extra_short.endswith(".") else extra_short
    except Exception as e:
        print("[Ollama-continue] Error:", e)
        return ""
    # t·ª± ƒë·ªông l·∫•y intent t·ª´ notion 
def get_all_categories():
    conn = sqlite3.connect(FAQ_DB_PATH)
    cur = conn.cursor()
    cur.execute("SELECT DISTINCT category FROM faq WHERE category IS NOT NULL")
    rows = cur.fetchall()
    conn.close()
    # tr·∫£ v·ªÅ list t√™n categories, lo·∫°i b·ªè None, r·ªóng
    cats = [ (r[0] or "").strip() for r in rows ]
    cats = [c for c in cats if c]
    cats.extend(["Th√¥ng tin ng√†nh", "Tra c·ª©u s√°ch"])
    return sorted(set(cats))
def get_all_major_names():
    conn = sqlite3.connect(FAQ_DB_PATH)
    cur = conn.cursor()
    cur.execute("SELECT name FROM majors")
    rows = cur.fetchall()
    conn.close()
    return [r[0].strip().lower() for r in rows]

def answer_from_majors(user_message: str) -> str:
    try:
        # --- 1. Tr√≠ch t√™n ng√†nh ---
        extract_prompt = f"""
B·∫°n l√† tr·ª£ l√Ω th∆∞ vi·ªán.
H√£y tr√≠ch t√™n NG√ÄNH t·ª´ c√¢u h·ªèi sau.
N·∫øu kh√¥ng t√¨m th·∫•y ng√†nh ‚Üí tr·∫£ v·ªÅ r·ªóng.

C√¢u h·ªèi: "{user_message}"

Ch·ªâ tr·∫£ v·ªÅ t√™n ng√†nh (vd: C√¥ng ngh·ªá th√¥ng tin, Kinh t·∫ø, CNTT).
Kh√¥ng gi·∫£i th√≠ch th√™m.
"""
        payload = {
            "model": OLLAMA_MODEL,
            "prompt": extract_prompt,
            "stream": False,
            "options": {"temperature": 0.0, "num_predict": 50}
        }
        r = requests.post(f"{OLLAMA_URL}/api/generate", json=payload, timeout=OLLAMA_TIMEOUT)

        major_key = (r.json().get("response") or "").strip().split("\n")[0]
        major_key = re.sub(r'[^0-9a-zA-Z√Ä-·ªπ\s]', '', major_key)

        if not major_key:
            return "M√¨nh ch∆∞a x√°c ƒë·ªãnh ƒë∆∞·ª£c ng√†nh trong c√¢u h·ªèi."

        # --- 2. T√¨m ng√†nh trong b·∫£ng majors ---
        conn = sqlite3.connect(FAQ_DB_PATH)
        cur = conn.cursor()
        cur.execute("""
            SELECT name, major_id, description
            FROM majors
            WHERE name LIKE ? OR major_id LIKE ?
        """, (f"%{major_key}%", f"%{major_key}%"))

        rows = cur.fetchall()
        conn.close()

        if not rows:
            return f"Kh√¥ng t√¨m th·∫•y ng√†nh li√™n quan: {major_key}"

        # format
        text = "\n".join(f"- {name} (M√£: {mid}): {desc}" for name, mid, desc in rows)

        # --- 3. Vi·∫øt c√¢u tr·∫£ l·ªùi ---
        answer_prompt = f"""
Ng∆∞·ªùi d√πng h·ªèi: "{user_message}"
D∆∞·ªõi ƒë√¢y l√† th√¥ng tin ng√†nh t√¨m ƒë∆∞·ª£c:

{text}

H√£y tr·∫£ l·ªùi t·ª± nhi√™n, KH√îNG b·ªãa th√™m.
"""
        payload2 = {
            "model": OLLAMA_MODEL,
            "prompt": answer_prompt,
            "stream": False,
            "options": {"temperature": 0.2, "num_predict": 150}
        }
        rr = requests.post(f"{OLLAMA_URL}/api/generate", json=payload2, timeout=OLLAMA_TIMEOUT)
        return (rr.json().get("response") or "").strip()

    except Exception as e:
        return f"[L·ªñI majors] {e}"


def _llm_format_books_answer(question: str, books: list[tuple], mode: str, extra_label: str = "") -> str:
    """
    D√πng Ollama ƒë·ªÉ vi·∫øt c√¢u tr·∫£ l·ªùi cho ƒë·∫πp, NH∆ØNG ch·ªâ d·ª±a tr√™n list `books`.
    books: list tuple (name, author, year, quantity, status, major_name)
    mode: 'book' | 'author' | 'major' | 'list'
    extra_label: t√™n t√°c gi·∫£ / t√™n ng√†nh / t√™n s√°ch g·ªëc n·∫øu mu·ªën nh·∫Øc l·∫°i
    """
    if not ollama_alive() or not books:
        return ""  # ƒë·ªÉ answer_from_books fallback sang format c·ª©ng

    # Gh√©p block s√°ch g·ª≠i cho LLM
    lines = []
    for idx, (name, author, year, qty, status, major_name) in enumerate(books, start=1):
        major_name = major_name or "Kh√¥ng r√µ"
        lines.append(
            f"{idx}) T√™n: {name} | T√°c gi·∫£: {author} | NƒÉm: {year} | "
            f"SL: {qty} | Tr·∫°ng th√°i: {status} | Ng√†nh: {major_name}"
        )
    books_block = "\n".join(lines)

    if mode == "book":
        mode_desc = "M·ªòT cu·ªën s√°ch c·ª• th·ªÉ m√† ng∆∞·ªùi d√πng ƒëang h·ªèi."
    elif mode == "author":
        mode_desc = f"c√°c s√°ch c·ªßa T√ÅC GI·∫¢ {extra_label}."
    elif mode == "major":
        mode_desc = f"c√°c s√°ch thu·ªôc NG√ÄNH {extra_label}."
    else:  # 'list'
        mode_desc = "DANH S√ÅCH c√°c s√°ch li√™n quan ƒë·∫øn c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng."

    system_prompt = f"""
B·∫°n l√† tr·ª£ l√Ω th∆∞ vi·ªán. B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p:
- C√ÇU H·ªéI c·ªßa ng∆∞·ªùi d√πng.
- DANH S√ÅCH S√ÅCH l·∫•y tr·ª±c ti·∫øp t·ª´ c∆° s·ªü d·ªØ li·ªáu th∆∞ vi·ªán.

NHI·ªÜM V·ª§:
1. D·ª±a v√†o DANH S√ÅCH S√ÅCH b√™n d∆∞·ªõi ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng v·ªÅ {mode_desc}.
2. CH·ªà ƒê∆Ø·ª¢C S·ª¨ D·ª§NG nh·ªØng s√°ch xu·∫•t hi·ªán trong danh s√°ch b√™n d∆∞·ªõi.
   **KH√îNG ƒê∆Ø·ª¢C B·ªäA TH√äM t√™n s√°ch, t√°c gi·∫£, nƒÉm, tr·∫°ng th√°i, s·ªë l∆∞·ª£ng, ng√†nh m·ªõi.**
3. N·∫øu danh s√°ch ch·ªâ c√≥ 1 s√°ch ‚Üí m√¥ t·∫£ chi ti·∫øt ch√≠nh cu·ªën ƒë√≥.
4. N·∫øu c√¢u h·ªèi l√† v·ªÅ NG√ÄNH ‚Üí CH·ªà ƒê∆Ø·ª¢C li·ªát k√™ c√°c s√°ch thu·ªôc ng√†nh ƒë√≥ trong danh s√°ch (kh√¥ng t·ª± t·∫°o th√™m).
5. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, t·ª± nhi√™n, ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu.

TUY·ªÜT ƒê·ªêI KH√îNG ƒê∆Ø·ª¢C:
- B·ªãa th√™m b·∫•t k·ª≥ cu·ªën s√°ch n√†o kh√¥ng c√≥ trong danh s√°ch.
- T·ª± t·∫°o t√™n s√°ch, t√°c gi·∫£, nƒÉm xu·∫•t b·∫£n.
- T·ª± t·∫°o th√™m n·ªôi dung m√¥ t·∫£ s√°ch n·∫øu danh s√°ch kh√¥ng cung c·∫•p.
- G·ªôp nh√≥m, th√™m s√°ch v√≠ d·ª• minh ho·∫° ngo√†i danh s√°ch.
- ƒê∆∞a ra g·ª£i √Ω kh√¥ng c√≥ trong d·ªØ li·ªáu.

N·∫æU DANH S√ÅCH CH·ªà C√ì 1 CU·ªêN ‚Üí ch·ªâ tr·∫£ ƒë√∫ng cu·ªën ƒë√≥.
N·∫æU DANH S√ÅCH C√ì NHI·ªÄU CU·ªêN ‚Üí CH·ªà LI·ªÜT K√ä NH·ªÆNG CU·ªêN ƒê√É CHO.
KH√îNG BAO GI·ªú LI·ªÜT K√ä TH√äM 3‚Äì7 CU·ªêN KH√ÅC T·ª∞ NGHƒ® RA.
"""

    user_prompt = f"""
C√¢u h·ªèi ng∆∞·ªùi d√πng: "{question}"

DANH S√ÅCH S√ÅCH T·ª™ C∆† S·ªû D·ªÆ LI·ªÜU:
{books_block}

H√£y tr·∫£ l·ªùi, NH·ªö: ch·ªâ d√πng th√¥ng tin trong danh s√°ch tr√™n.
"""

    payload = {
        "model": OLLAMA_MODEL,
        "prompt": system_prompt + "\n\n" + user_prompt,
        "stream": False,
        "options": {
            "temperature": 0.1,   # gi·∫£m max b·ªãa
            "num_predict": 400
        }
    }

    try:
        r = requests.post(f"{OLLAMA_URL.rstrip('/')}/api/generate",
                          json=payload, timeout=OLLAMA_TIMEOUT)
        if r.status_code != 200:
            print("[books-llm-format] HTTP", r.status_code, r.text[:200])
            return ""
        resp = (r.json().get("response") or "").strip()
        return resp
    except Exception as e:
        print("[books-llm-format] Error:", e)
        return ""

MAJOR_EMB = []       # danh s√°ch vector
MAJOR_META = []      # (name, major_id, description)
def vector(txt: str):
    if not embed_model:
        return None
    return embed_model.encode(txt, normalize_embeddings=True)
def build_major_embedding_index():
    global MAJOR_EMB, MAJOR_META

    conn = sqlite3.connect(FAQ_DB_PATH)
    cur = conn.cursor()
    cur.execute("SELECT name, major_id, description FROM majors")
    rows = cur.fetchall()
    conn.close()

    MAJOR_META = rows
    MAJOR_EMB = [vector(r[0]) for r in rows]

def search_majors_by_embedding(query: str, top_k=1):
    if not embed_model or not MAJOR_EMB:
        return []
    qv = vector(query)
    sims = np.dot(MAJOR_EMB, qv)
    idx = np.argsort(sims)[::-1][:top_k]
    return [(i, sims[i], MAJOR_META[i]) for i in idx]

# ====== EMBEDDING CHO BOOKS (SEMANTIC SEARCH) ======
EMB_MODEL_NAME_BOOKS = os.getenv(
    "BOOK_EMB_MODEL",
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"  # ho·∫∑c model kh√°c b·∫°n th√≠ch
)
book_emb_model = SentenceTransformer(EMB_MODEL_NAME_BOOKS)
# Model embedding cho majors
MAJOR_EMB_MODEL = os.getenv(
    "MAJOR_EMB_MODEL",
    "keepitreal/vietnamese-sbert"
)
major_emb_model = SentenceTransformer(MAJOR_EMB_MODEL)
print("[Books-Emb] Loading SentenceTransformer model:", EMB_MODEL_NAME_BOOKS)
book_emb_model = SentenceTransformer(EMB_MODEL_NAME_BOOKS)

# Cache: embeddings + d·ªØ li·ªáu th√¥ c·ªßa books
BOOK_EMBS: np.ndarray | None = None   # shape (N_books, dim)
BOOK_ROWS: list[tuple] | None = None  # [(name, author, year, qty, status, major_name), ...]


def build_book_embedding_index() -> tuple[np.ndarray, list[tuple]]:
    """
    ƒê·ªçc to√†n b·ªô b·∫£ng books + majors v√† build index embedding cho S√ÅCH.
    Ch·ªâ build 1 l·∫ßn, sau ƒë√≥ d√πng l·∫°i t·ª´ cache.
    """
    global BOOK_EMBS, BOOK_ROWS

    # N·∫øu ƒë√£ build tr∆∞·ªõc ƒë√≥ r·ªìi th√¨ d√πng l·∫°i
    if BOOK_EMBS is not None and BOOK_ROWS is not None:
        return BOOK_EMBS, BOOK_ROWS

    conn = sqlite3.connect(FAQ_DB_PATH)
    cur = conn.cursor()
    cur.execute("""
        SELECT b.name, b.author, b.year, b.quantity, b.status, m.name
        FROM books b
        LEFT JOIN majors m ON b.major_id = m.major_id
    """)
    rows = cur.fetchall()
    conn.close()

    if not rows:
        dim = book_emb_model.get_sentence_embedding_dimension()
        BOOK_EMBS = np.zeros((0, dim), dtype=np.float32)
        BOOK_ROWS = []
        return BOOK_EMBS, BOOK_ROWS

    # Chu·∫©n b·ªã text m√¥ t·∫£ m·ªói cu·ªën s√°ch ƒë·ªÉ embedding
    texts = []
    for (name, author, year, qty, status, major_name) in rows:
        name = name or ""
        author = author or ""
        major_name = major_name or ""
        year = str(year or "")
        status = status or ""
        t = (
            f"S√°ch: {name}. T√°c gi·∫£: {author}. Ng√†nh: {major_name}. "
            f"NƒÉm: {year}. Ch·ªß ƒë·ªÅ: {name} {major_name} {author}"
        )
        texts.append(t)

    print(f"[Books-Emb] Building embeddings cho {len(texts)} s√°ch...")
    emb = book_emb_model.encode(
        texts,
        convert_to_numpy=True,
        normalize_embeddings=True,  # ƒë·ªÉ cosine = dot
        show_progress_bar=False,
    )

    BOOK_EMBS = emb
    BOOK_ROWS = rows
    print("[Books-Emb] Done.")
    return BOOK_EMBS, BOOK_ROWS


def search_books_by_embedding(
    query: str,
    top_k: int = 10,
    min_sim: float = 0.45,
) -> list[tuple[tuple, float]]:
    """
    T√¨m s√°ch theo NGHƒ®A b·∫±ng cosine similarity.
    Tr·∫£ v·ªÅ list[(row, sim)] ƒë√£ sort gi·∫£m d·∫ßn.
    row ƒë√∫ng c·∫•u tr√∫c:
        (name, author, year, quantity, status, major_name)
    """
    emb, rows = build_book_embedding_index()
    if emb.shape[0] == 0:
        return []

    q_vec = book_emb_model.encode(
        [query],
        convert_to_numpy=True,
        normalize_embeddings=True,
    )[0]

    sims = emb @ q_vec  # cosine v√¨ ƒë√£ normalize
    idx_sorted = np.argsort(-sims)

    results: list[tuple[tuple, float]] = []
    for i in idx_sorted[:top_k]:
        sim = float(sims[i])
        if sim < min_sim:
            continue
        results.append((rows[i], sim))
    return results


def answer_from_books(user_message: str) -> str:
    """
    Tra c·ª©u s√°ch d·ª±a tr√™n EMBEDDING (semantic search),
    sau ƒë√≥ (n·∫øu ƒë∆∞·ª£c) nh·ªù Ollama vi·∫øt l·∫°i c√¢u tr·∫£ l·ªùi cho t·ª± nhi√™n h∆°n.

    Kh√¥ng d√πng keyword, kh√¥ng d√πng fuzzy cho BOOK n·ªØa.
    """
    try:
        text_raw = (user_message or "").strip()
        if not text_raw:
            return "M√¨nh ch∆∞a nh·∫≠n ƒë∆∞·ª£c n·ªôi dung ƒë·ªÉ tra c·ª©u s√°ch."

        # L·∫•y top s√°ch theo NGHƒ®A
        results = search_books_by_embedding(text_raw, top_k=12, min_sim=0.45)
        if not results:
            return (
                "Hi·ªán m√¨nh ch∆∞a t√¨m ƒë∆∞·ª£c s√°ch ph√π h·ª£p trong danh m·ª•c. "
                "B·∫°n th·ª≠ ghi r√µ h∆°n t√™n s√°ch, t√°c gi·∫£ ho·∫∑c ng√†nh nh√©."
            )

        # T√°ch rows & similarity
        rows = [r[0] for r in results]
        sims = [r[1] for r in results]

        # Xem c√¢u h·ªèi c√≥ d·∫°ng "li·ªát k√™ / t·∫•t c·∫£" kh√¥ng
        text_norm = normalize_vi(text_raw)
        list_keywords = [
            "tat ca", "t·∫•t c·∫£",
            "liet ke", "li·ªát k√™",
            "danh sach", "danh s√°ch",
            "sach lien quan", "s√°ch li√™n quan",
            "cac sach", "c√°c s√°ch",
            "nhung sach", "nh·ªØng s√°ch",
        ]
        is_list_query = any(k in text_norm for k in list_keywords)

        # N·∫øu c√¢u h·ªèi ki·ªÉu li·ªát k√™ ‚Üí ƒë∆∞a list cho LLM
        if is_list_query or len(rows) > 3:
            books = rows  # list[(name, author, year, qty, status, major_name)]
            llm_ans = _llm_format_books_answer(
                text_raw,
                books,
                mode="list",
            )
            if llm_ans:
                return llm_ans

            # Fallback: li·ªát k√™ c·ª©ng
            block = "\n".join(
                f"- {n} ‚Äì {a}, {y}, SL: {q}, Tr·∫°ng th√°i: {s}, Ng√†nh: {mj or 'Kh√¥ng r√µ'}"
                for (n, a, y, q, s, mj) in books
            )
            return f"D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë s√°ch li√™n quan ƒë·∫øn c√¢u h·ªèi c·ªßa b·∫°n:\n\n{block}"

        # Ng∆∞·ª£c l·∫°i: coi l√† h·ªèi 1 CU·ªêN S√ÅCH G·∫¶N NH·∫§T
        best_row = rows[0]
        best_sim = sims[0]
        global LAST_BOOK_CONTEXT
        LAST_BOOK_CONTEXT = best_row

        if best_sim < 0.5:
            return (
                "M√¨nh ch∆∞a ch·∫Øc s√°ch n√†o ph√π h·ª£p v·ªõi c√¢u h·ªèi n√†y. "
                "B·∫°n th·ª≠ n√™u r√µ t√™n s√°ch, t√°c gi·∫£ ho·∫∑c m√¥ t·∫£ chi ti·∫øt h∆°n nh√©."
            )

        n, a, y, q, s, mj = best_row
        major_label = mj or "Kh√¥ng r√µ"

        # Cho LLM format ƒë·∫πp h∆°n (mode 'book')
        llm_ans = _llm_format_books_answer(
            text_raw,
            [best_row],
            mode="book",
            extra_label=n,
        )
        if llm_ans:
            return llm_ans

        # Fallback: format c·ª©ng
        return (
            f"**Th√¥ng tin s√°ch g·∫ßn nh·∫•t v·ªõi c√¢u h·ªèi c·ªßa b·∫°n:**\n"
            f"- T√™n: {n}\n"
            f"- T√°c gi·∫£: {a}\n"
            f"- NƒÉm XB: {y}\n"
            f"- S·ªë l∆∞·ª£ng: {q}\n"
            f"- Tr·∫°ng th√°i: {s}\n"
            f"- Ng√†nh: {major_label}"
        )

    except Exception as e:
        return f"[L·ªñI books-emb] {e}"




def classify_category(user_message: str) -> str:
    """
    Ph√¢n lo·∫°i intent d√πng LLM.
    ∆Øu ti√™n LLM ‚Üí fallback rule nh·∫π n·∫øu LLM tr·∫£ linh tinh.
    KH√îNG fuzzy, KH√îNG rule √©p c·ª©ng nh∆∞ tr∆∞·ªõc.
    """

    msg = (user_message or "").strip()
    if not msg:
        return "Tra c·ª©u s√°ch"

    # ===== 1. L·∫•y category th·∫≠t trong FAQ =====
    try:
        conn = sqlite3.connect(FAQ_DB_PATH)
        cur = conn.cursor()
        cur.execute("SELECT DISTINCT category FROM faq WHERE category IS NOT NULL")
        rows = cur.fetchall()
        conn.close()
        faq_categories = [(r[0] or "").strip() for r in rows if r[0]]
    except:
        faq_categories = []

    faq_categories = sorted(set([c for c in faq_categories if c]))

    # Category c·ªë ƒë·ªãnh
    special = ["Tra c·ª©u s√°ch", "Th√¥ng tin ng√†nh"]
    allowed = special + faq_categories

    def norm(s: str) -> str:
        return normalize_vi((s or "").strip())

    categories_list_str = "\n".join(f"- {c}" for c in allowed)

    system_prompt = f"""
B·∫°n l√† tr·ª£ l√Ω th∆∞ vi·ªán.
Nhi·ªám v·ª•: ph√¢n lo·∫°i c√¢u h·ªèi v√†o ƒê√öNG M·ªòT category trong danh s√°ch sau:

{categories_list_str}

QUY T·∫ÆC:
- V·ªÅ s√°ch ‚Üí "Tra c·ª©u s√°ch".
- V·ªÅ ng√†nh h·ªçc ‚Üí "Th√¥ng tin ng√†nh".
- V·ªÅ quy ƒë·ªãnh, th·ªß t·ª•c, nhi·ªám v·ª•, ch·ª©c nƒÉng, gi·ªù m·ªü c·ª≠a, n·ªôi quy ‚Üí ch·ªçn ƒë√∫ng category trong FAQ.
- KH√îNG b·ªãa th√™m category m·ªõi.
- Ch·ªâ ƒë∆∞·ª£c tr·∫£ v·ªÅ ƒë√∫ng 1 category, kh√¥ng th√™m ch·ªØ n√†o.
"""

    payload = {
        "model": OLLAMA_MODEL,
        "prompt": system_prompt + "\n\nC√¢u h·ªèi: " + msg,
        "stream": False,
        "options": {"temperature": 0.0, "num_predict": 32},
    }

    # ===== 2. G·ªçi LLM =====
    try:
        r = requests.post(
            f"{OLLAMA_URL.rstrip('/')}/api/generate",
            json=payload,
            timeout=OLLAMA_TIMEOUT
        )
        raw = (r.json().get("response") or "").strip().splitlines()[0]
        c = raw.strip().lstrip("-‚Ä¢* ").strip('"').strip("'")
        c_norm = norm(c)

        # N·∫øu LLM tr·∫£ ƒë√∫ng ‚Üí OK
        for cat in allowed:
            if c_norm == norm(cat):
                return cat

    except Exception as e:
        print("[classify_category] LLM error:", e)

    # ===== 3. Fallback rule (NH·∫∏, KH√îNG √©p sai FAQ) =====
    msg_n = norm(msg)

    # h·ªèi ng√†nh
    if any(k in msg_n for k in ["nganh", "chuyen nganh", "ma nganh", "hoc nganh"]):
        return "Th√¥ng tin ng√†nh"

    # h·ªèi s√°ch
    if any(k in msg_n for k in ["sach", "gi√°o tr√¨nh", "giao trinh", "tai lieu"]):
        return "Tra c·ª©u s√°ch"

    # cu·ªëi c√πng ‚Üí cho v·ªÅ s√°ch (an to√†n nh·∫•t)
    return "Tra c·ª©u s√°ch"



def detect_book_followup_intent(user_message: str) -> str:
    """
    D√πng LLM ƒë·ªÉ hi·ªÉu c√¢u h·ªèi ti·∫øp theo ƒëang h·ªèi g√¨ v·ªÅ cu·ªën s√°ch trong LAST_BOOK_CONTEXT.
    Tr·∫£ v·ªÅ 1 trong:
    - 'quantity' : h·ªèi v·ªÅ s·ªë l∆∞·ª£ng, c√≤n nhi·ªÅu kh√¥ng, c√≤n bao nhi√™u quy·ªÉn, v.v.
    - 'status'   : h·ªèi ki·ªÉu c√≤n h√†ng kh√¥ng, t√¨nh tr·∫°ng ra sao, c√≥ s·∫µn kh√¥ng,...
    - 'other'    : h·ªèi c√°i kh√°c nh∆∞ng v·∫´n li√™n quan cu·ªën s√°ch (vd: n·ªôi dung, kh√≥/d·ªÖ,...)
    - 'none'     : kh√¥ng li√™n quan t·ªõi cu·ªën s√°ch tr∆∞·ªõc.
    """
    global LAST_BOOK_CONTEXT
    if not ollama_alive() or LAST_BOOK_CONTEXT is None:
        return "none"

    n, a, y, q, s, mj = LAST_BOOK_CONTEXT
    book_info = (
        f"T√™n: {n}. T√°c gi·∫£: {a}. NƒÉm: {y}. "
        f"S·ªë l∆∞·ª£ng: {q}. Tr·∫°ng th√°i: {s}. Ng√†nh: {mj or 'Kh√¥ng r√µ'}."
    )

    system_prompt = """
B·∫°n l√† tr·ª£ l√Ω th∆∞ vi·ªán.
B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c:
- Th√¥ng tin m·ªôt cu·ªën s√°ch.
- C√¢u h·ªèi m·ªõi c·ªßa ng∆∞·ªùi d√πng (sau khi h·ªç v·ª´a h·ªèi v·ªÅ cu·ªën s√°ch n√†y).

NHI·ªÜM V·ª§:
Hi·ªÉu ng·ªØ nghƒ©a c√¢u h·ªèi m·ªõi v√† ph√¢n lo·∫°i n√≥ th√†nh ƒë√∫ng 1 nh√£n sau:

- "quantity"  ‚Üí n·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ s·ªë L∆Ø·ª¢NG, c√≤n bao nhi√™u quy·ªÉn, c√≤n nhi·ªÅu kh√¥ng, h·∫øt ch∆∞a,...
- "status"    ‚Üí n·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ T√åNH TR·∫†NG / C√íN H√ÄNG KH√îNG, c√≥ s·∫µn ƒë·ªÉ m∆∞·ª£n kh√¥ng,...
- "other"     ‚Üí n·∫øu ng∆∞·ªùi d√πng h·ªèi th·ª© kh√°c nh∆∞ng v·∫´n LI√äN QUAN cu·ªën s√°ch (n·ªôi dung, ƒë·ªô kh√≥, n√™n h·ªçc,...).
- "none"      ‚Üí n·∫øu c√¢u h·ªèi KH√îNG li√™n quan t·ªõi cu·ªën s√°ch tr∆∞·ªõc.

Ch·ªâ ƒë∆∞·ª£c tr·∫£ v·ªÅ DUY NH·∫§T m·ªôt t·ª´ trong 4 t·ª´ sau:
quantity
status
other
none
"""

    user_prompt = f"""
Th√¥ng tin s√°ch:
{book_info}

C√¢u h·ªèi m·ªõi c·ªßa ng∆∞·ªùi d√πng: "{user_message}"

H√£y tr·∫£ v·ªÅ ƒë√∫ng 1 t·ª´ trong: quantity, status, other, none.
"""

    payload = {
        "model": OLLAMA_MODEL,
        "prompt": system_prompt + "\n\n" + user_prompt,
        "stream": False,
        "options": {"temperature": 0.0, "num_predict": 10},
    }

    try:
        r = requests.post(f"{OLLAMA_URL.rstrip('/')}/api/generate",
                          json=payload, timeout=OLLAMA_TIMEOUT)
        raw = (r.json().get("response") or "").strip().splitlines()[0].strip().lower()
        if raw in ("quantity", "status", "other", "none"):
            return raw
        return "none"
    except Exception as e:
        print("[followup-intent] error:", e)
        return "none"
def process_message(sentence: str) -> str:
    sentence = (sentence or "").strip()
    if not sentence:
        return "Xin l·ªói, m√¨nh ch∆∞a hi·ªÉu √Ω b·∫°n."

    reply: Optional[str] = None
    tag_to_log: Optional[str] = None
    confidence: float = 0.0
    text_norm = normalize_vi(sentence)

    global LAST_BOOK_CONTEXT

    # ====== B∆Ø·ªöC 1: x·ª≠ l√Ω c√¢u h·ªèi ti·∫øp theo v·ªÅ CU·ªêN S√ÅCH tr∆∞·ªõc ƒë√≥ ======
    if LAST_BOOK_CONTEXT is not None and ollama_alive():
        intent = detect_book_followup_intent(sentence)  # quantity | status | other | none

        if intent in ("quantity", "status", "other"):
            n, a, y, q, s, mj = LAST_BOOK_CONTEXT
            major_label = mj or "Kh√¥ng r√µ"

            try:
                qty = int(q)
            except Exception:
                qty = None

            if intent == "quantity":
                if qty is None:
                    reply = (
                        f"M√¨nh ch∆∞a c√≥ d·ªØ li·ªáu ch√≠nh x√°c v·ªÅ s·ªë l∆∞·ª£ng s√°ch **{n}**.\n"
                        f"- Tr·∫°ng th√°i hi·ªán t·∫°i: {s}."
                    )
                else:
                    if qty > 0:
                        reply = (
                            f"S√°ch **{n}** c·ªßa {a} hi·ªán trong h·ªá th·ªëng c√≤n kho·∫£ng {qty} quy·ªÉn.\n"
                            f"Tr·∫°ng th√°i: {s}."
                        )
                    else:
                        reply = f"S√°ch **{n}** c·ªßa {a} hi·ªán ƒë√£ h·∫øt h√†ng trong h·ªá th·ªëng."

            elif intent == "status":
                if qty is not None and qty > 0:
                    reply = (
                        f"S√°ch **{n}** c·ªßa {a} hi·ªán ƒëang c√≤n trong th∆∞ vi·ªán "
                        f"(kho·∫£ng {qty} quy·ªÉn). Tr·∫°ng th√°i: {s}."
                    )
                else:
                    reply = (
                        f"S√°ch **{n}** c·ªßa {a} hi·ªán kh√¥ng c√≤n s·∫µn trong kho ho·∫∑c s·ªë l∆∞·ª£ng r·∫•t √≠t.\n"
                        f"Tr·∫°ng th√°i ghi nh·∫≠n: {s}."
                    )

            elif intent == "other":
                reply = (
                    f"B·∫°n ƒëang h·ªèi th√™m v·ªÅ s√°ch **{n}** c·ªßa {a} (nƒÉm {y}, ng√†nh {major_label}).\n"
                    f"Hi·ªán h·ªá th·ªëng ch·ªâ l∆∞u th√¥ng tin c∆° b·∫£n: s·ªë l∆∞·ª£ng = {q}, tr·∫°ng th√°i = {s}. "
                    f"N·∫øu b·∫°n c·∫ßn n·ªôi dung chi ti·∫øt, b·∫°n c√≥ th·ªÉ tra c·ª©u s√°ch tr·ª±c ti·∫øp t·∫°i th∆∞ vi·ªán."
                )

            if reply:
                tag_to_log = "Tra c·ª©u s√°ch (followup)"

    # ====== B∆Ø·ªöC 2: Router ch√≠nh ======
    if reply is None:
        if ollama_alive():
            # ----- 2A. D√πng LLM ph√¢n lo·∫°i tr∆∞·ªõc -----
            try:
                category = classify_category(sentence)
            except Exception as e:
                print("[process_message] classify_category error:", e)
                category = None

            if category:
                tag_to_log = category

            # 2A.1. N·∫øu LLM n√≥i ƒë√¢y l√† c√¢u h·ªèi v·ªÅ NG√ÄNH
            if category == "Th√¥ng tin ng√†nh":
                reply = answer_from_majors(sentence)
                tag_to_log = "Th√¥ng tin ng√†nh"

            # 2A.2. N·∫øu LLM n√≥i ƒë√¢y l√† c√¢u h·ªèi v·ªÅ S√ÅCH
            elif category == "Tra c·ª©u s√°ch":
                reply = answer_from_books(sentence)
                tag_to_log = "Tra c·ª©u s√°ch"

            # 2A.3. C√≤n l·∫°i: xem nh∆∞ FAQ (Quy ƒë·ªãnh, Nhi·ªám v·ª•, Ch·ª©c nƒÉng, ...)
            # 2A.3. X·ª≠ l√Ω FAQ ‚Äî kh√¥ng b·ªãa, ch·ªâ d√πng d·ªØ li·ªáu SQLite
            elif category and category not in ("Tra c·ª©u s√°ch", "Th√¥ng tin ng√†nh"):
                try:
                    conn_faq = sqlite3.connect(FAQ_DB_PATH)
                    cur = conn_faq.cursor()
                    cur.execute("""
                        SELECT question, answer
                        FROM faq
                        WHERE category = ?
                        AND (approved = 1 OR approved IS NULL)
                    """, (category,))
                    rows = cur.fetchall()
                    conn_faq.close()
                except Exception as e:
                    print("[FAQ SELECT error]", e)
                    rows = []

                if rows:
                    # Gh√©p block
                    faq_block = "\n\n".join(
                        f"{idx}) Q: {(q or '').strip()}\n   A: {(a or '').strip()}"
                        for idx, (q, a) in enumerate(rows, 1)
                    )

                    answer_prompt = f"""
            B·∫°n l√† tr·ª£ l√Ω th∆∞ vi·ªán.
            Ch·ªâ ƒë∆∞·ª£c d√πng N·ªòI DUNG c√≥ trong danh s√°ch Answer d∆∞·ªõi ƒë√¢y.
            KH√îNG ƒê∆Ø·ª¢C b·ªãa th√™m th√¥ng tin m·ªõi.
            KH√îNG ƒë∆∞·ª£c ƒë∆∞a v√≠ d·ª• kh√¥ng n·∫±m trong danh s√°ch.
            N·∫øu kh√¥ng c√≥ Answer ph√π h·ª£p ‚Üí ph·∫£i tr·∫£ l·ªùi:
            "Hi·ªán t·∫°i m√¨nh ch∆∞a c√≥ th√¥ng tin ch√≠nh x√°c trong h·ªá th·ªëng th∆∞ vi·ªán v·ªÅ c√¢u h·ªèi n√†y."

            C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:
            {sentence}

            Danh s√°ch Answer theo category "{category}":

            {faq_block}

            H√£y tr·∫£ l·ªùi ƒë√∫ng n·ªôi dung, KH√îNG m·ªü r·ªông ra ngo√†i.
            """

                    try:
                        payload = {
                            "model": OLLAMA_MODEL,
                            "prompt": answer_prompt,
                            "stream": False,
                            "options": {"temperature": 0.1, "num_predict": 200}
                        }
                        r = requests.post(
                            f"{OLLAMA_URL.rstrip('/')}/api/generate",
                            json=payload,
                            timeout=OLLAMA_TIMEOUT
                        )
                        if r.status_code == 200:
                            raw = (r.json().get("response") or "").strip()
                            # N·∫øu LLM b·ªãa ngo√†i d·ªØ li·ªáu ‚Üí ph√°t hi·ªán v√† ch·∫∑n l·∫°i
                            if not raw or any(x in raw.lower() for x in [
                                "v√≠ d·ª•", "v√≠ du", "example", "theo m√¨nh", "m√¨nh nghƒ©"
                            ]):
                                reply = ("Hi·ªán t·∫°i m√¨nh ch∆∞a c√≥ th√¥ng tin ch√≠nh x√°c "
                                        "trong h·ªá th·ªëng th∆∞ vi·ªán v·ªÅ c√¢u h·ªèi n√†y.")
                            else:
                                reply = raw
                            confidence = 0.9
                    except Exception as e:
                        print("[FAQ LLM error]", e)

                # kh√¥ng t√¨m ƒë∆∞·ª£c -> fallback
                if reply is None or not reply.strip():
                    reply = ("Hi·ªán t·∫°i m√¨nh ch∆∞a c√≥ th√¥ng tin ch√≠nh x√°c "
                            "trong h·ªá th·ªëng th∆∞ vi·ªán v·ªÅ c√¢u h·ªèi n√†y.")
                    confidence = 0.5

            # ----- 2B. N·∫øu LLM/FAQ kh√¥ng cho ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi ‚Üí fallback embedding nh∆∞ c≈© -----

                if book_hits:
                    reply = answer_from_books(sentence)
                    tag_to_log = tag_to_log or "Tra c·ª©u s√°ch"

                # majors embedding
                if reply is None or not reply.strip():
                    try:
                        major_hits = search_majors_by_embedding(sentence, top_k=1)
                    except Exception as e:
                        print("[process_message] major-emb error:", e)
                        major_hits = []

                    if major_hits and major_hits[0][1] >= 0.55:
                        reply = answer_from_majors(sentence)
                        tag_to_log = tag_to_log or "Th√¥ng tin ng√†nh"

        else:
            # ====== Ollama kh√¥ng s·ªëng ‚Üí fallback thu·∫ßn embedding (kh√¥ng FAQ) ======
            try:
                book_hits = search_books_by_embedding(sentence, top_k=1, min_sim=0.55)
            except Exception as e:
                print("[process_message] book-emb (no LLM) error:", e)
                book_hits = []

            if book_hits:
                reply = answer_from_books(sentence)
                tag_to_log = "Tra c·ª©u s√°ch"
            else:
                try:
                    major_hits = search_majors_by_embedding(sentence, top_k=1)
                except Exception as e:
                    print("[process_message] major-emb (no LLM) error:", e)
                    major_hits = []

                if major_hits and major_hits[0][1] >= 0.55:
                    reply = answer_from_majors(sentence)
                    tag_to_log = "Th√¥ng tin ng√†nh"

    # ====== Fallback n·∫øu v·∫´n ch∆∞a c√≥ c√¢u tr·∫£ l·ªùi ======
    if reply is None or not reply.strip():
        reply = "Xin l·ªói, m√¨nh ch∆∞a hi·ªÉu √Ω b·∫°n."
        confidence = 0.0

    # ====== Append th√™m cho m∆∞·ª£t (n·∫øu b·∫≠t) ======
    if ENABLE_OLLAMA_APPEND and reply.strip() and ollama_alive():
        extra = ollama_generate_continuation(reply, sentence, max_sentences=3)
        if extra:
            reply = f"{reply.strip()} {extra.strip()}"

    # ====== Ghi log v√†o conversations ======
    conn = ensure_main_db()
    cur = conn.cursor()
    cur.execute(
        "INSERT INTO conversations(user_message, bot_reply, intent_tag, confidence, time) "
        "VALUES (?,?,?,?,?)",
        (sentence, reply, tag_to_log, confidence, _now()),
    )
    conn.commit()
    conn.close()

    # ====== Ghi th√™m v√†o faq.db (inbox) ======
    try:
        log_question_for_notion(f"User: {sentence}\nBot: {reply}")
    except Exception as e:
        print(f"[Notion inbox] L·ªói ghi faq.db: {e}")

    # ====== ƒê·∫©y l√™n Notion (background) ======
    should_push = (
        LOG_ALL_QUESTIONS
        or reply.strip().startswith("Xin l·ªói, m√¨nh ch∆∞a hi·ªÉu")
        or confidence < CONF_THRESHOLD
        or tag_to_log is None
    )
    if should_push:
        try:
            threading.Thread(target=push_to_notion, args=(sentence, reply), daemon=True).start()
        except Exception as e:
            print("Notion push error:", e)

    return reply



# ============== CLI ==============
def _test_push_notion_once():
    token, dbid, mode, base = _resolve_notion_env()
    tok_prefix = (token.split("_",1)[0]+"_") if "_" in token else token[:6]
    print("[TEST] mode:", mode, "| dbid:", dbid, "| base:", base, "| token_prefix:", tok_prefix)

    # Test /status (Cloudflare/Notion)
    try:
        r = requests.get("https://api.notion.com/v1/status", timeout=6)
        print("[TEST] status api.notion.com:", r.status_code)
    except Exception as e:
        print("[TEST] status error:", e)

    if not token or not dbid:
        print("[TEST] Thi·∫øu token/dbid")
        return

    # T·∫°o payload ƒë·ªông ƒë√∫ng schema th·ª±c t·∫ø c·ªßa database
    q = "Ping t·ª´ script"
    a = "N·∫øu th·∫•y page n√†y l√† OK."
    try:
        payload = _build_dynamic_payload_force(dbid, q, a) 
    except Exception as e:
        print(f"[TEST] Build payload error:", e)
        return

    ok, code, body = _http_create_page(token, base, payload, timeout_s=15.0)
    print(f"[TEST] POST {base}/pages ‚Üí", code, (body[:200] if isinstance(body, str) else body))



if __name__ == "__main__":
    print("ü§ñ Chatbot ƒë√£ s·∫µn s√†ng! G√µ 'quit' ƒë·ªÉ tho√°t.")
    conn = ensure_main_db()
    cur  = conn.cursor()
    build_book_embedding_index()
    build_major_embedding_index()

    #_test_push_notion_once()
    try:
        while True:
            sentence = input("B·∫°n: ").strip()
            if sentence.lower() == "quit":
                break
            print("Bot:", process_message(sentence))
    finally:
        conn.close()