import random
import json
import torch
from model import NeuralNet
from nltk_utils import tokenize, bag_of_words
import numpy as np
import sqlite3, datetime, os
from state_manager import StateManager
import requests  # NEW
# --------------------# ---- OLLAMA AUGMENT (append th√™m c√¢u tr·∫£ l·ªùi) ----
USE_OLLAMA_AUGMENT = True           # b·∫≠t/t·∫Øt t√≠nh nƒÉng b·ªï sung
OLLAMA_MODEL = "qwen2:1.5b"         # ho·∫∑c "llama3.2:3b"
OLLAMA_URL = "http://127.0.0.1:11434/api/generate"

# C·∫§U H√åNH L∆ØU LOG
# --------------------
CHAT_DB_PATH = "chat.db"

# Inbox c√¢u h·ªèi ƒë·ªÉ ƒë·∫©y l√™n Notion
FAQ_DB_PATH = "D:/HTML/chat2/rag/faqs.db"   # gi·ªØ nguy√™n nh∆∞ push_logs.py

CONF_THRESHOLD = 0.60  # ng∆∞·ª°ng t·ª± tin intent
LOG_ALL_QUESTIONS = True  # True = log m·ªçi c√¢u; False = ch·ªâ log khi bot ch∆∞a hi·ªÉu / t·ª± tin th·∫•p

# --------------------
# DB: conversations (chat.db)
# --------------------
conn = sqlite3.connect(CHAT_DB_PATH)
cur = conn.cursor()
cur.execute("""
CREATE TABLE IF NOT EXISTS conversations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_message TEXT,
    bot_reply   TEXT,
    intent_tag  TEXT,
    confidence  REAL,
    time        TEXT
);
""")
conn.commit()

# kh·ªüi t·∫°o gi√∫p chat bot c√≥ cu·ªôc tr√≤ chuy·ªán

# --------------------
# DB: questions_log (faqs.db) - t·∫°o n·∫øu ch∆∞a c√≥
# --------------------
def ensure_questions_log():
    os.makedirs(os.path.dirname(FAQ_DB_PATH), exist_ok=True)
    conn2 = sqlite3.connect(FAQ_DB_PATH)
    cur2 = conn2.cursor()
    cur2.execute("""
    CREATE TABLE IF NOT EXISTS questions_log (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        question   TEXT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        synced     INTEGER DEFAULT 0
    )
    """)
    conn2.commit(); conn2.close()
# N√≥i ng·∫Øn: ƒë√¢y l√† ch·ªó l∆∞u ‚Äúh·ªôp th∆∞ ƒë·∫øn‚Äù cho c√°c c√¢u h·ªèi m√† chatbot ch∆∞a ƒë·ªìng b·ªô l√™n Notion.
def log_question_for_notation(question: str):
    """Ghi 1 c√¢u h·ªèi v√†o 'inbox' ƒë·ªÉ push l√™n Notion sau n√†y (push_logs.py)."""
    if not question or not question.strip():
        return
    ensure_questions_log()
    conn2 = sqlite3.connect(FAQ_DB_PATH)
    cur2 = conn2.cursor()
    cur2.execute("INSERT INTO questions_log (question, synced) VALUES (?, 0)", (question.strip(),))
    conn2.commit(); conn2.close()
# üìå T√≥m l·∫°i: ƒë√¢y l√† h√†m ƒë∆∞a c√¢u h·ªèi v√†o h√†ng ch·ªù (inbox). Sau n√†y script push_logs.py s·∫Ω ƒë·ªçc c√°c b·∫£n ghi synced = 0 trong b·∫£ng n√†y v√† ƒë·∫©y l√™n Notion.
# --------------------
# MODEL
# --------------------
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

with open('intents.json', 'r', encoding='utf-8-sig') as f:
    intents = json.load(f)

FILE = "data.pth"
data = torch.load(FILE, map_location=device)

input_size  = data["input_size"]
hidden_size = data["hidden_size"]
output_size = data["output_size"]
all_words   = data["all_words"]
tags        = data["tags"]
model_state = data["model_state"]

model = NeuralNet(input_size, hidden_size, output_size).to(device)
model.load_state_dict(model_state)
model.eval()

# --------------------
# STATE / FLOW
# --------------------

def ollama_generate_append(base_reply: str, user_message: str) -> str:
    """
    G·ªçi Ollama ƒë·ªÉ sinh th√™m c√¢u tr·∫£ l·ªùi d·ª±a tr√™n base_reply v√† user_message.
    Tr·∫£ v·ªÅ chu·ªói b·ªï sung ho·∫∑c "" n·∫øu l·ªói/kh√¥ng c√≥ g√¨.
    """
    try:
        payload = {
            "model": OLLAMA_MODEL,
            # ch·ªâ truy·ªÅn tr·ª±c ti·∫øp 2 bi·∫øn m√† kh√¥ng k√®m prompt h∆∞·ªõng d·∫´n d√†i
            "prompt": f"Ng∆∞·ªùi d√πng: {user_message}\nC√¢u tr·∫£ l·ªùi hi·ªán c√≥: {base_reply}\n\nVi·∫øt th√™m:",
            "stream": False,
            "options": {
                "num_predict": 220
            }
        }
        r = requests.post(OLLAMA_URL, json=payload, timeout=25)
        if r.ok:
            txt = (r.json().get("response") or "").strip()
            if txt and txt.lower() not in (base_reply.lower()):
                return txt
    except Exception:
        pass
    return ""

try:
    state_mgr = StateManager("flows.json")
except Exception:
    state_mgr = StateManager()

INTERRUPT_INTENTS = set()
CANCEL_WORDS = {"h·ªßy","hu·ª∑","huy","cancel","tho√°t","d·ª´ng","ƒë·ªïi ch·ªß ƒë·ªÅ","doi chu de"}
# ng·∫Øt h·ªôi tho
print("ü§ñ Chatbot ƒë√£ s·∫µn s√†ng! G√µ 'quit' ƒë·ªÉ tho√°t.")

try:
    while True:
        sentence = input("B·∫°n: ").strip()
        if sentence.lower() == "quit":
            break

        # H·ªßy flow th·ªß c√¥ng
        if sentence.lower() in CANCEL_WORDS:
            try:
                state_mgr.exit_flow()
            except Exception:
                pass
            reply = "ƒê√£ h·ªßy lu·ªìng hi·ªán t·∫°i. B·∫°n mu·ªën h·ªèi g√¨ ti·∫øp?"
            print("Bot:", reply)
            cur.execute(
                "INSERT INTO conversations(user_message, bot_reply, intent_tag, confidence, time) VALUES (?,?,?,?,?)",
                (sentence, reply, None, 0.0, datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
            )
            conn.commit()
            continue

        reply = None
        tag_to_log = None
        confidence = 0.0

        # --- NLU: d·ª± ƒëo√°n intent ---
        tokens = tokenize(sentence)
        X = bag_of_words(tokens, all_words)
        X = torch.from_numpy(X).unsqueeze(0).to(device)

        with torch.no_grad():
            output = model(X)
            probs = torch.softmax(output, dim=1)
            prob, pred_idx = torch.max(probs, dim=1)
            tag = tags[pred_idx.item()]
            confidence = float(prob.item())

        # --- ∆ØU TI√äN NG·ªÆ C·∫¢NH ---
        if getattr(state_mgr, "active_flow", None):
            try:
                ctx_reply = state_mgr.handle(tag, sentence)
            except Exception:
                ctx_reply = None
            if ctx_reply:
                reply = ctx_reply
                tag_to_log = tag

        if reply is None and confidence > CONF_THRESHOLD:
            try:
                ctx_reply = state_mgr.handle(tag, sentence)
            except Exception:
                ctx_reply = None
            if ctx_reply:
                reply = ctx_reply
                tag_to_log = tag

        if reply is None:
            try:
                boot = state_mgr.bootstrap_by_text(sentence)
            except Exception:
                boot = None
            if boot:
                reply = boot

        if reply is None and confidence > CONF_THRESHOLD:
            resp_list = next((it["responses"] for it in intents["intents"] if it["tag"] == tag), None)
            if resp_list:
                reply = random.choice(resp_list)
                tag_to_log = tag

        if reply is None:
            reply = "Xin l·ªói, m√¨nh ch∆∞a hi·ªÉu √Ω b·∫°n."

        print("Bot:", reply)

        # L∆ØU LOG H·ªòI THO·∫†I (chat.db)
        cur.execute(
            "INSERT INTO conversations(user_message, bot_reply, intent_tag, confidence, time) VALUES (?,?,?,?,?)",
            (sentence, reply, tag_to_log, confidence, datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        )
        conn.commit()

        # GHI "INBOX C√ÇU H·ªéI" ƒê·ªÇ ƒê·∫®Y L√äN NOTION
        should_push_to_notion = (
            LOG_ALL_QUESTIONS or
            reply.strip().startswith("Xin l·ªói, m√¨nh ch∆∞a hi·ªÉu") or
            confidence < CONF_THRESHOLD or
            tag_to_log is None
        )
        if should_push_to_notion:
            try:
                # ‚úÖ g·ªçi H√ÄM, kh√¥ng ph·∫£i g√°n bi·∫øn
                log_question_for_notation(f"User: {sentence}\nBot: {reply}")

            except Exception:
                pass

finally:
    conn.close()
