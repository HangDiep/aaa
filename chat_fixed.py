
import os, random, json, sqlite3, datetime
#chat_fixed.py
import numpy as np
import torch, requests
from model import NeuralNet
from nltk_utils import tokenize, bag_of_words
from state_manager import StateManager
import threading
from dotenv import load_dotenv
from notion_client import Client
from typing import Optional, List, Dict
ENV_PATH = r"D:/HTML/chat2/rag/.env"
try:
    if os.path.exists(ENV_PATH):
        load_dotenv(ENV_PATH)
except Exception:
    pass
_notion_cached = None

# C√≥ th·ªÉ ƒë·∫∑t trong .env (∆∞u ti√™n .env) ho·∫∑c d√πng default d∆∞·ªõi ƒë√¢y
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://127.0.0.1:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "qwen2:1.5b")  # ƒë·ªïi th√†nh model b·∫°n ƒë√£ pull
OLLAMA_TIMEOUT = float(os.getenv("OLLAMA_TIMEOUT", "20"))  # gi√¢y
ENABLE_OLLAMA_APPEND = True  # b·∫≠t/t·∫Øt vi·ªác cho Ollama vi·∫øt th√™m
MAX_OLLAMA_APPEND_TOKENS = 150  # s·ªë token t·ªëi ƒëa Ollama ƒë∆∞·ª£c vi·∫øt th√™m
FAQ_API_URL = None
INVENTORY_API_URL = None

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
#ng d√πng h·ªèi bot tr·∫£ l·ªùi l∆∞u v√†o chat.db
CHAT_DB_PATH = os.path.join(BASE_DIR, "chat.db")
print(f"[ChatDB] Using: {CHAT_DB_PATH}")

DB_PATH = CHAT_DB_PATH  # d√πng ƒë√∫ng ƒë∆∞·ªùng d·∫´n DB
#Ghi c√°c c√¢u h·ªèi ‚Äúch∆∞a hi·ªÉu‚Äù ho·∫∑c ‚Äúch·ªù duy·ªát‚Äù
FAQ_DB_PATH = os.path.normpath("D:/HTML/chat2/rag/faqs.db")
CONF_THRESHOLD = 0.60
LOG_ALL_QUESTIONS = True

FAQ_API_URL = "http://localhost:8000/search"
INVENTORY_API_URL = "http://localhost:8000/inventory"

INTERRUPT_INTENTS = set()
CANCEL_WORDS = {"h·ªßy", "hu·ª∑", "huy", "cancel", "tho√°t", "d·ª´ng", "ƒë·ªïi ch·ªß ƒë·ªÅ", "doi chu de"}

# =========================
# DB helpers
# =========================
def ensure_main_db() -> sqlite3.Connection:
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True) if os.path.dirname(DB_PATH) else None
    conn = sqlite3.connect(DB_PATH, check_same_thread=False)
    cur = conn.cursor()
    cur.execute(
        """
        CREATE TABLE IF NOT EXISTS conversations (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_message TEXT,
            bot_reply   TEXT,
            intent_tag  TEXT,
            confidence  REAL,
            time        TEXT
        );
        """
    )
    conn.commit()
    return conn

def ensure_questions_log_db() -> None:
    dir_name = os.path.dirname(FAQ_DB_PATH)
    if dir_name and not os.path.exists(dir_name):
        os.makedirs(dir_name, exist_ok=True)
    conn2 = sqlite3.connect(FAQ_DB_PATH)
    cur2 = conn2.cursor()
    cur2.execute(
        """
        CREATE TABLE IF NOT EXISTS questions_log (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            question   TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            synced     INTEGER DEFAULT 0
        )
        """
    )
    conn2.commit()
    conn2.close()

def log_question_for_notion(question: str) -> None:
    if not question or not question.strip():
        return
    ensure_questions_log_db()
    conn2 = sqlite3.connect(FAQ_DB_PATH)
    cur2 = conn2.cursor()
    cur2.execute(
        "INSERT INTO questions_log (question, synced) VALUES (?, 0)",
        (question.strip(),),
    )
    conn2.commit()
    conn2.close()


# =========================
# Model load
# =========================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

with open("intents.json", "r", encoding="utf-8-sig") as f:
    intents = json.load(f)

_data = torch.load("data.pth", map_location=device)
input_size  = _data["input_size"]
hidden_size = _data["hidden_size"]
output_size = _data["output_size"]
all_words   = _data["all_words"]
tags        = _data["tags"]
model_state = _data["model_state"]

model = NeuralNet(input_size, hidden_size, output_size).to(device)
model.load_state_dict(model_state)
model.eval()

try:
    state_mgr = StateManager("flows.json")
except Exception:
    state_mgr = StateManager()

def _now():
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def get_faq_response(sentence: str) -> Optional[str]:
    """
    G·ªçi FAQ API v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ d·∫°ng b·∫£ng text ƒë·∫πp,
    thay v√¨ JSON th√¥.
    """
    try:
        resp = requests.get(FAQ_API_URL, params={"q": sentence}, timeout=5)
        if resp.status_code != 200:
            print(f"[FAQ] HTTP {resp.status_code}: {resp.text[:200]}")
            return None
        
        data = resp.json()
        if not isinstance(data, list) or not data:
            return None

        # D·ª±ng b·∫£ng text
        lines: List[str] = []
        lines.append("üìñ **K·∫øt qu·∫£ FAQ:**\n")
        lines.append("| C√¢u h·ªèi | Tr·∫£ l·ªùi |")
        lines.append("|---------|---------|")

        for item in data:
            q = item.get("question", "").strip()
            a = item.get("answer", "").strip()
            if q or a:
                # Escape k√Ω t·ª± '|' ƒë·ªÉ kh√¥ng ph√° b·∫£ng
                q = q.replace("|", "ÔΩú")
                a = a.replace("|", "ÔΩú")
                lines.append(f"| {q} | {a} |")

        return "\n".join(lines) if len(lines) > 3 else None

    except requests.RequestException as e:
        print(f"[FAQ] L·ªói k·∫øt n·ªëi API: {e}")
        return None
    except Exception as e:
        print(f"[FAQ] L·ªói x·ª≠ l√Ω d·ªØ li·ªáu: {e}")
        return None

def get_inventory_response(sentence: str) -> Optional[str]:
    try:
        resp = requests.get(INVENTORY_API_URL, params={"book_name": sentence}, timeout=5)
        if resp.status_code != 200:
            print(f"[Inventory] HTTP {resp.status_code}: {resp.text[:200]}")
            return None
        data = resp.json()
        if isinstance(data, list) and data:
            book = data[0]
            name = book.get("name")
            author = book.get("author", "?")
            year = book.get("year", "?")
            quantity = book.get("quantity", "?")
            status = book.get("status", "?")
            if name:
                return (
                    f"S√°ch '{name}' c·ªßa t√°c gi·∫£ {author}, nƒÉm xu·∫•t b·∫£n {year}, "
                    f"s·ªë l∆∞·ª£ng: {quantity}, tr·∫°ng th√°i: {status}"
                )
        return None
    except requests.RequestException as e:
        print(f"[Inventory] L·ªói k·∫øt n·ªëi API: {e}")
        return None
    except Exception as e:
        print(f"[Inventory] L·ªói x·ª≠ l√Ω d·ªØ li·ªáu: {e}")
        return None
# =========================
# CORE: x·ª≠ l√Ω 1 c√¢u (web/CLI d√πng chung)
# =========================
def process_message(sentence: str) -> str:
    sentence = (sentence or "").strip()
    if not sentence:
        return "Xin l·ªói, m√¨nh ch∆∞a hi·ªÉu √Ω b·∫°n."

    lower_sentence = sentence.lower()

    # KH·ªûI T·∫†O BI·∫æN TR∆Ø·ªöC KHI D√ôNG
    reply: Optional[str] = None
    tag_to_log: Optional[str] = None
    confidence: float = 0.0
    if reply is None or not str(reply).strip():
        reply = "Xin l·ªói, m√¨nh ch∆∞a hi·ªÉu √Ω b·∫°n."
    fallback_reply = "Xin l·ªói, m√¨nh ch∆∞a hi·ªÉu √Ω b·∫°n."
    if ENABLE_OLLAMA_APPEND and reply.strip() and reply.strip() != fallback_reply:
        base_reply = reply
        try:
            extra = ollama_generate_append(base_reply, sentence)
            if extra and extra.strip() and extra.strip() not in base_reply:
                reply = f"{base_reply.strip()} {extra.strip()}"
            else:
                reply = base_reply
        except Exception:
            reply = base_reply


    # L∆∞u log + push Notion (gi·ªØ nguy√™n nh∆∞ b·∫°n ƒëang l√†m)
    conn = ensure_main_db(); cur = conn.cursor()
    cur.execute(
        "INSERT INTO conversations(user_message, bot_reply, intent_tag, confidence, time) VALUES (?,?,?,?,?)",
        (sentence, reply, tag_to_log, confidence, _now()),
    )
    conn.commit(); conn.close()

    should_push = (
        LOG_ALL_QUESTIONS
        or reply.strip().startswith("Xin l·ªói, m√¨nh ch∆∞a hi·ªÉu")
        or confidence < CONF_THRESHOLD
        or tag_to_log is None
    )
    if should_push:
        try:
            threading.Thread(target=push_to_notion, args=(sentence, reply), daemon=True).start()
        except Exception as e:
            print("Notion push error:", e)

    return reply
def _get_notion_client():
    """
    Lazy-init Notion Client t·ª´ .env. N·∫øu thi·∫øu token/DBID -> tr·∫£ v·ªÅ None (kh√¥ng ch·∫∑n lu·ªìng chat).
    """
    global _notion_cached
    if _notion_cached is not None:
        return _notion_cached

def _get_notion_client():
    """
    Lazy-init Notion Client t·ª´ .env. N·∫øu thi·∫øu token/DBID -> tr·∫£ v·ªÅ None (kh√¥ng ch·∫∑n lu·ªìng chat).
    """
    global _notion_cached
    if _notion_cached is not None:
        return _notion_cached

    try:
        token = os.getenv("NOTION_TOKEN")
        dbid  = os.getenv("NOTION_DATABASE_ID")
        if token and dbid:
            _notion_cached = (Client(auth=token), dbid)
        else:
            print("‚ö†Ô∏è NOTION_TOKEN/NOTION_DATABASE_ID ch∆∞a c√≥ trong .env ho·∫∑c .env kh√¥ng t·ªìn t·∫°i.")
            _notion_cached = None
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói kh·ªüi t·∫°o Notion Client: {e}")
        _notion_cached = None
    return _notion_cached

def _rt(txt: str):
    return [{"type": "text", "text": {"content": txt or ""}}]

def push_to_notion(q: str, a: str):
    """
    ƒê·∫©y Q/A l√™n Notion. Kh√¥ng raise l·ªói ra ngo√†i, ƒë·ªÉ tr√°nh l√†m h·ªèng lu·ªìng tr·∫£ l·ªùi.
    """
    pair = _get_notion_client()
    if not pair:
        return
    client, dbid = pair
    q = (q or "").strip()
    a = (a or "").strip()
    if not q:
        return
    try:
        client.pages.create(
            parent={"database_id": dbid},
            properties={
                "Question": {"rich_text": _rt(q)},
                "Answer":   {"rich_text": _rt(a)},
                "Approved": {"checkbox": False},
                "Language": {"select": {"name": "Ti·∫øng Vi·ªát"}},
            },
        )
        # d√πng properties theo ƒë√∫ng schema DB c·ªßa b·∫°n
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi t·∫°o page Notion: {e}")
def ollama_generate_append(base_reply: str, user_message: str) -> str:
    """
    G·ªçi Ollama ƒë·ªÉ VI·∫æT TH√äM 1‚Äì3 c√¢u ti·∫øng Vi·ªát, b√°m ng·ªØ c·∫£nh th∆∞ vi·ªán.
    Kh√¥ng thay th·∫ø n·ªôi dung ch√≠nh; tr√°nh b·ªãa v√† KH√îNG m√¢u thu·∫´n d·ªØ ki·ªán c√≥ s·∫µn.
    Tr·∫£ v·ªÅ chu·ªói b·ªï sung ho·∫∑c "" n·∫øu l·ªói/kh√¥ng c√≥ g√¨.
    """
    if not ENABLE_OLLAMA_APPEND:
        return ""

    system_prompt = (
        "B·∫°n l√† tr·ª£ l√Ω TH∆Ø VI·ªÜN Tr∆∞·ªùng ƒê·∫°i h·ªçc T√¢y Nguy√™n (DHTN).\n"
        "- Ch·ªâ B·ªî SUNG 1‚Äì2 c√¢u, ng·∫Øn g·ªçn, b√°m C√ÇU TR·∫¢ L·ªúI G·ªêC.\n"
        "- Ch·ªâ n√≥i v·ªÅ: gi·ªù m·ªü/ƒë√≥ng, m∆∞·ª£n‚Äìtr·∫£, th·∫ª th∆∞ vi·ªán, quy ƒë·ªãnh, ph√≠ ph·∫°t, tra c·ª©u, khu s√°ch, li√™n h·ªá.\n"
        "- N·∫øu kh√¥ng ch·∫Øc li√™n quan th∆∞ vi·ªán: TR·∫¢ V·ªÄ CHU·ªñI R·ªñNG.\n"
        "- KH√îNG b·ªãa, KH√îNG qu·∫£ng c√°o, KH√îNG tr·∫£ l·ªùi c√¢u c√° nh√¢n/ngo√†i ph·∫°m vi.\n"
        "- Ch·ªâ TI·∫æNG VI·ªÜT. KH√îNG chuy·ªÉn ng√¥n ng·ªØ kh√°c.\n"
        "- KH√îNG ch√†o h·ªèi x√£ giao, KH√îNG d√πng ngo·∫∑c k√©p, KH√îNG c·∫£m th√°n."
    )


    # D√πng /api/generate c·ªßa Ollama (ƒë∆°n gi·∫£n, latency th·∫•p)
    url = f"{OLLAMA_URL.rstrip('/')}/api/generate"
    payload = {
    "model": OLLAMA_MODEL,
    "prompt": f"{system_prompt}\n\nNg∆∞·ªùi d√πng: {user_message}\nC√¢u tr·∫£ l·ªùi g·ªëc:\n{base_reply}\n\nY√™u c·∫ßu: B·ªï sung 1‚Äì2 c√¢u. N·∫øu kh√¥ng ph√π h·ª£p, tr·∫£ v·ªÅ tr·ªëng.",
    "stream": False,
    "options": {
        "temperature": 0.1,          # b·ªõt bay
        "top_p": 0.9,
        "repeat_penalty": 1.2,       # h·∫°n ch·∫ø l·∫∑p
        "num_predict": 80,           # ng·∫Øn g·ªçn
        "stop": ["\n\n", "\"", "‚Äù", "‚Äú"]  # ch·∫∑n xu·ªëng d√≤ng d√†i, ngo·∫∑c k√©p
    }
}

    try:
        r = requests.post(url, json=payload, timeout=OLLAMA_TIMEOUT)
        if r.status_code != 200:
            print(f"[Ollama] HTTP {r.status_code}: {r.text[:200]}")
            return ""
        data = r.json()  # {"model": "...", "created_at": "...", "response": "...", ...}
        extra = (data.get("response") or "").strip()
        # L·ªçc b·ªõt m√¥ t·∫£ th·ª´a
        if not extra:
            return ""
        # Ch·∫∑n vi·ªác l·∫∑p l·∫°i y nguy√™n reply ch√≠nh
        if extra in base_reply:
            return ""
        # R√∫t g·ªçn 1‚Äì3 c√¢u (ph√≤ng tr∆∞·ªùng h·ª£p model vi·∫øt d√†i)
        # T√°ch theo d·∫•u ch·∫•m. N·∫øu th·∫•y xu·ªëng d√≤ng, gh√©p l·∫°i.
        sentences = [s.strip() for s in extra.replace("\n", " ").split(".") if s.strip()]
        if not sentences:
            return ""
        extra_short = ". ".join(sentences[:3]).strip()
        if extra_short and not extra_short.endswith("."):
            extra_short += "."
        extra_short = sanitize_vi(extra_short)
        if not extra_short:
            return ""
        return extra_short
    except requests.RequestException as e:
        print(f"[Ollama] L·ªói k·∫øt n·ªëi: {e}")
        return ""
    except Exception as e:
        print(f"[Ollama] L·ªói x·ª≠ l√Ω: {e}")
        return ""
import re

def sanitize_vi(extra: str) -> str:
    if not extra: return ""
    # b·ªè k√Ω t·ª± CJK/emoji
    extra = re.sub(r'[\u3400-\u9FFF\uF900-\uFAFF]+', '', extra)
    extra = re.sub(r'[\U0001F300-\U0001FAFF]', '', extra)
    # b·ªè ngo·∫∑c k√©p + kho·∫£ng tr·∫Øng th·ª´a
    extra = extra.replace('‚Äú','').replace('‚Äù','').replace('"','').strip()
    extra = re.sub(r'\s+', ' ', extra)
    # b·ªè c√¢u ch√†o/ x√£ giao
    banned_starts = ("ch√†o m·ª´ng", "r·∫•t ti·∫øc", "xin ch√†o", "c·∫£m ∆°n")
    if extra.lower().startswith(banned_starts): return ""
    # qu√° ng·∫Øn/ v√¥ nghƒ©a
    if len(extra.split()) < 3: return ""
    return extra

# =========================
# CLI ch·ªâ ch·∫°y khi g·ªçi tr·ª±c ti·∫øp file
# =========================
if __name__ == "__main__":
    print("ü§ñ Chatbot ƒë√£ s·∫µn s√†ng! G√µ 'quit' ƒë·ªÉ tho√°t.")
    conn = ensure_main_db()
    cur  = conn.cursor()
    try:
        while True:
            sentence = input("B·∫°n: ").strip()
            if sentence.lower() == "quit":
                break
            print("Bot:", process_message(sentence))
    finally:
        conn.close()