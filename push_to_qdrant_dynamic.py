"""
Qdrant Incremental Sync Script (Single Collection Architecture)

- Má»—i báº£ng trong SQLite tÆ°Æ¡ng á»©ng vá»›i source_table trong Qdrant.
- Khi sync má»™t báº£ng:
    + Upsert táº¥t cáº£ row hiá»‡n cÃ³ trong SQLite (chá»‰ row approved = 1, trá»« báº£ng 'nganh').
    + Láº¥y danh sÃ¡ch ID hiá»‡n Ä‘ang cÃ³ trong Qdrant cho báº£ng Ä‘Ã³.
    + XoÃ¡ cÃ¡c ID Ä‘Ã£ cÃ³ trong Qdrant nhÆ°ng khÃ´ng cÃ²n trong SQLite (Ä‘Ã£ xoÃ¡ / unapproved trÃªn Notion).
=> KhÃ´ng cáº§n xoÃ¡ sáº¡ch cáº£ báº£ng trong Qdrant, chá»‰ xoÃ¡ Ä‘Ãºng record "má»“ cÃ´i".
"""

import os
import sys
import sqlite3
import gc  # Garbage Collector

from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import (
    Distance,
    VectorParams,
    PointStruct,
    Filter,
    FieldCondition,
    MatchValue,
)
from dotenv import load_dotenv

# ==========================
#  Load env
# ==========================

ENV_PATH = r"D:\HTML\a - Copy\rag\.env"
try:
    if os.path.exists(ENV_PATH):
        load_dotenv(ENV_PATH, override=True)
    else:
        load_dotenv()
except Exception:
    pass

FAQ_DB_PATH = os.getenv("FAQ_DB_PATH", r"D:\HTML\a - Copy\faq.db")
QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")

GLOBAL_COLLECTION_NAME = "knowledge_base"
BATCH_SIZE = 32  # batch embed


# ==========================
#  Helper
# ==========================

def normalize(x: str) -> str:
    if not x:
        return ""
    return " ".join(str(x).lower().strip().split())

def get_table_description_from_sqlite(table_name: str) -> str:
    try:
        conn = sqlite3.connect(FAQ_DB_PATH)
        cur = conn.cursor()
        cur.execute("SELECT description FROM collections_config WHERE name=?", (table_name,))
        row = cur.fetchone()
        conn.close()
        return row[0] if row else f"MÃ´ táº£ báº£ng {table_name}"
    except:
        return f"MÃ´ táº£ báº£ng {table_name}"

def get_db_connection():
    return sqlite3.connect(FAQ_DB_PATH)


def build_embed_text(row_dict: dict, table_name: str) -> str:
    """
    Táº¡o text Ä‘á»ƒ embed. Æ¯u tiÃªn cÃ¡c trÆ°á»ng quan trá»ng.
    """
    skip_cols = ["notion_id", "last_updated", "approved"]
    priority_cols = [
        "name",
        "title",
        "question",
        "ten",
        "tieu_de",
        "cau_hoi",
        "noidung",
        "content",
        "answer",
    ]

    parts = [f"Chá»§ Ä‘á»: {table_name}"]

    for col in priority_cols:
        col_lower = col.lower()
        if col_lower in row_dict and row_dict[col_lower]:
            parts.append(str(row_dict[col_lower]))

    for col, value in row_dict.items():
        if col not in skip_cols and col.lower() not in priority_cols and value:
            parts.append(f"{col}: {value}")

    return normalize(" ".join(parts))


def row_generator(cursor, batch_size=100):
    """Äá»c tá»«ng cá»¥c dá»¯ liá»‡u tá»« SQLite, trÃ¡nh full RAM."""
    while True:
        rows = cursor.fetchmany(batch_size)
        if not rows:
            break
        yield rows


def get_existing_ids_in_qdrant(client: QdrantClient, table_name: str):
    """
    Láº¥y toÃ n bá»™ ID (notion_id) hiá»‡n Ä‘ang cÃ³ trong Qdrant cho source_table = table_name.
    DÃ¹ng scroll vá»›i filter.
    """
    existing_ids = set()
    offset = None

    while True:
        points, offset = client.scroll(
            collection_name=GLOBAL_COLLECTION_NAME,
            scroll_filter=Filter(
                must=[
                    FieldCondition(
                        key="source_table",
                        match=MatchValue(value=table_name),
                    )
                ]
            ),
            limit=1000,
            offset=offset,
            with_payload=False,
            with_vectors=False,
        )

        for p in points:
            existing_ids.add(str(p.id))

        if offset is None:
            break

    print(f"  ðŸ”Ž Qdrant currently has {len(existing_ids)} ids for table '{table_name}'")
    return existing_ids


# ==========================
#  Qdrant Sync per Table
# ==========================

def sync_table_to_global_collection(table_name: str, embed_model, client: QdrantClient):
    """
    Incremental sync cho 1 báº£ng:

    1. Äá»c dá»¯ liá»‡u hiá»‡n táº¡i tá»« SQLite:
        - Náº¿u cÃ³ cá»™t approved & báº£ng != 'nganh' â†’ chá»‰ láº¥y approved = 1
        - NgÆ°á»£c láº¡i â†’ láº¥y táº¥t cáº£.
    2. Upsert embedding cho táº¥t cáº£ row Ä‘Ã³ vÃ o Qdrant.
    3. Láº¥y danh sÃ¡ch ID Ä‘ang cÃ³ trong Qdrant (theo source_table).
    4. XoÃ¡ cÃ¡c ID cÃ³ trong Qdrant nhÆ°ng khÃ´ng cÃ²n trong SQLite.
    """

    print(f"\n[SYNC] Processing table: {table_name.upper()}")

    # 1. Láº¥y danh sÃ¡ch ID hiá»‡n Ä‘ang cÃ³ trong Qdrant cho báº£ng nÃ y
    existing_ids = get_existing_ids_in_qdrant(client, table_name)

    # 2. Äá»c dá»¯ liá»‡u tá»« SQLite
    conn = get_db_connection()
    cur = conn.cursor()

    try:
        cur.execute(f"PRAGMA table_info({table_name})")
        columns_info = cur.fetchall()
        if not columns_info:
            print(f"  âŒ Table '{table_name}' not found in SQLite.")
            conn.close()
            return
        columns = [col[1] for col in columns_info]
    except Exception as e:
        print(f"  âŒ Error reading table info for '{table_name}': {e}")
        conn.close()
        return

    lower_columns = [c.lower() for c in columns]
    has_approved = "approved" in lower_columns
    lower_table_name = table_name.lower()

    if has_approved and lower_table_name != "nganh":
        sql_query = f"SELECT * FROM {table_name} WHERE approved = 1"
    else:
        sql_query = f"SELECT * FROM {table_name}"

    print(f"  ðŸ”Ž SQL: {sql_query}")
    try:
        cur.execute(sql_query)
    except Exception as e:
        print(f"  âŒ Error executing query on '{table_name}': {e}")
        conn.close()
        return

    total_synced = 0
    points_buffer = []
    sqlite_ids = set()  # lÆ°u láº¡i toÃ n bá»™ notion_id hiá»‡n cÃ³ trong SQLite cho báº£ng nÃ y

    for rows_chunk in row_generator(cur, batch_size=BATCH_SIZE):
        texts_to_embed = []
        payloads = []
        ids = []

        for row in rows_chunk:
            row_dict = dict(zip(columns, row))
            notion_id = row_dict.get("notion_id")

            if not notion_id:
                continue

            notion_id_str = str(notion_id)
            sqlite_ids.add(notion_id_str)

            text = build_embed_text(row_dict, table_name)

            payload = {k: v for k, v in row_dict.items() if k != "notion_id"}
            payload["source_table"] = table_name
            # ðŸ”¥ NEW: Gáº¯n mÃ´ táº£ báº£ng vÃ o Qdrant
            description = get_table_description_from_sqlite(table_name)
            payload["table_description"] = description
            ids.append(notion_id_str)
            texts_to_embed.append(text)
            payloads.append(payload)

        if texts_to_embed:
            try:
                embeddings = embed_model.encode(
                    texts_to_embed, normalize_embeddings=True
                )

                for i, _id in enumerate(ids):
                    points_buffer.append(
                        PointStruct(
                            id=_id,
                            vector=embeddings[i].tolist(),
                            payload=payloads[i],
                        )
                    )

                if len(points_buffer) >= BATCH_SIZE:
                    client.upsert(
                        collection_name=GLOBAL_COLLECTION_NAME,
                        points=points_buffer,
                    )
                    total_synced += len(points_buffer)
                    print(f"  ðŸ’¾ Upserted {len(points_buffer)} items...", end="\r")
                    points_buffer = []
                    gc.collect()

            except Exception as e:
                print(f"  âš ï¸ Error embedding batch: {e}")

    if points_buffer:
        client.upsert(
            collection_name=GLOBAL_COLLECTION_NAME,
            points=points_buffer,
        )
        total_synced += len(points_buffer)

    conn.close()
    print(f"\n  âœ… Finished upserting {total_synced} items from '{table_name}'")

    # 3. XoÃ¡ cÃ¡c ID "má»“ cÃ´i" trong Qdrant (cÃ³ trong Qdrant nhÆ°ng khÃ´ng cÃ²n trong SQLite)
    ids_to_delete = existing_ids - sqlite_ids
    if ids_to_delete:
        print(
            f"  ðŸ—‘ï¸ Deleting {len(ids_to_delete)} obsolete points in Qdrant for table '{table_name}'..."
        )
        try:
            client.delete(
                collection_name=GLOBAL_COLLECTION_NAME,
                points_selector=list(ids_to_delete),  # qdrant-client nháº­n list id trá»±c tiáº¿p
            )
            print("  âœ… Obsolete points deleted from Qdrant.")
        except Exception as e:
            print(f"  âš ï¸ Error deleting obsolete points: {e}")
    else:
        print("  âœ” No obsolete points to delete in Qdrant.")

    gc.collect()


# ==========================
#  Collection Init & Cleanup
# ==========================

def init_global_collection(client: QdrantClient):
    """Khá»Ÿi táº¡o collection duy nháº¥t náº¿u chÆ°a cÃ³"""
    try:
        if not client.collection_exists(GLOBAL_COLLECTION_NAME):
            print(f"Creating global collection: {GLOBAL_COLLECTION_NAME}")
            client.create_collection(
                collection_name=GLOBAL_COLLECTION_NAME,
                vectors_config=VectorParams(size=1024, distance=Distance.COSINE),
            )
            client.create_payload_index(
                collection_name=GLOBAL_COLLECTION_NAME,
                field_name="source_table",
                field_schema="keyword",
            )
            print("âœ… Collection & Index created.")
        else:
            print(f"Existing collection found: {GLOBAL_COLLECTION_NAME}")
    except Exception as e:
        print(f"Error checking/creating collection: {e}")


def cleanup_old_collections(client: QdrantClient):
    """
    Optional: XÃ³a cÃ¡c collections cÅ© láº» táº» Ä‘á»ƒ dá»n rÃ¡c (náº¿u trÆ°á»›c Ä‘Ã¢y dÃ¹ng nhiá»u collection)
    """
    try:
        collections = client.get_collections().collections
        for c in collections:
            if c.name != GLOBAL_COLLECTION_NAME:
                print(f"ðŸ—‘ï¸ Deleting old fragmented collection: {c.name}")
                client.delete_collection(c.name)
    except Exception as e:
        print(f"Warning cleaning up: {e}")


# ==========================
#  Main
# ==========================

def main():
    print("ðŸš€ Incremental Qdrant Sync Started...")

    # 1. Load Model
    print("ðŸ“¦ Loading embedding model...")
    try:
        embed_model = SentenceTransformer("BAAI/bge-m3", device="cpu")
    except Exception:
        embed_model = SentenceTransformer("keepitreal/vietnamese-sbert")

    # 2. Connect Qdrant
    print(f"ðŸ”— Connecting to Qdrant ({QDRANT_URL})...")
    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)

    # 3. Init Global Collection
    init_global_collection(client)

    # 4. Optional: cleanup collections cÅ©
    # cleanup_old_collections(client)

    # 5. Sync
    conn = get_db_connection()
    cur = conn.cursor()
    cur.execute("SELECT name FROM sqlite_master WHERE type='table' ORDER BY name")
    all_tables = [
        row[0] for row in cur.fetchall() if not row[0].startswith("sqlite_")
    ]
    conn.close()

    # Náº¿u script Ä‘Æ°á»£c gá»i vá»›i tÃªn báº£ng â†’ chá»‰ sync báº£ng Ä‘Ã³
    specific_table = sys.argv[1] if len(sys.argv) > 1 else None

    if specific_table:
        print(f"ðŸ“Œ Running in single-table mode: {specific_table}")
        sync_table_to_global_collection(specific_table, embed_model, client)
    else:
        print(f"ðŸ“‹ Found {len(all_tables)} tables to sync.")
        for table in all_tables:
            sync_table_to_global_collection(table, embed_model, client)
            gc.collect()

    print("\nðŸŽ‰ INCREMENTAL SYNC COMPLETED SUCCESSFULLY!")


if __name__ == "__main__":
    main()
