"""
Enterprise Grade Qdrant Sync Script (Single Collection Architecture)
Tá»± Ä‘á»™ng embed vÃ  push táº¥t cáº£ báº£ng vÃ o má»™t Collection duy nháº¥t 'knowledge_base'
Tá»‘i Æ°u hÃ³a bá»™ nhá»›: DÃ¹ng Batch Processing & Generators Ä‘á»ƒ khÃ´ng load háº¿t vÃ o RAM
"""

import os
import sys
import sqlite3
import numpy as np
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue
from dotenv import load_dotenv
import gc  # Garbage Collector

# Load .env
ENV_PATH = r"D:\HTML\a - Copy\rag\.env"
try:
    if os.path.exists(ENV_PATH):
        load_dotenv(ENV_PATH, override=True)
    else:
        load_dotenv()
except Exception:
    pass

FAQ_DB_PATH = os.getenv("FAQ_DB_PATH", r"D:\HTML\a - Copy\faq.db")
QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")

# TÃªn Collection duy nháº¥t cho toÃ n bá»™ há»‡ thá»‘ng
GLOBAL_COLLECTION_NAME = "knowledge_base"
BATCH_SIZE = 32  # Sá»‘ lÆ°á»£ng vector embed má»—i láº§n (giáº£m xuá»‘ng náº¿u RAM yáº¿u)

def normalize(x: str) -> str:
    if not x: return ""
    return " ".join(str(x).lower().strip().split())

def get_db_connection():
    return sqlite3.connect(FAQ_DB_PATH)

def build_embed_text(row_dict: dict, table_name: str) -> str:
    """
    Táº¡o text Ä‘á»ƒ embed. Æ¯u tiÃªn cÃ¡c trÆ°á»ng quan trá»ng.
    """
    skip_cols = ["notion_id", "last_updated", "approved"]
    priority_cols = ["name", "title", "question", "ten", "tieu_de", "cau_hoi", "noidung", "content", "answer"]
    
    parts = [f"Chá»§ Ä‘á»: {table_name}"]  # Context injection
    
    # ThÃªm cÃ¡c cá»™t Æ°u tiÃªn
    for col in priority_cols:
        col_lower = col.lower()
        if col_lower in row_dict and row_dict[col_lower]:
            parts.append(str(row_dict[col_lower]))
            
    # ThÃªm cÃ¡c cá»™t cÃ²n láº¡i
    for col, value in row_dict.items():
        if col not in skip_cols and col.lower() not in priority_cols and value:
            parts.append(f"{col}: {value}")
            
    return normalize(" ".join(parts))

def row_generator(cursor, batch_size=100):
    """Memory-safe generator: Äá»c tá»«ng cá»¥c dá»¯ liá»‡u tá»« SQLite"""
    while True:
        rows = cursor.fetchmany(batch_size)
        if not rows:
            break
        yield rows

def sync_table_to_global_collection(table_name: str, embed_model, client):
    """
    Sync dá»¯ liá»‡u tá»« báº£ng table_name vÃ o 'knowledge_base'
    """
    print(f"\n[SYNC] Processing table: {table_name.upper()}")
    
    conn = get_db_connection()
    cur = conn.cursor()
    
    # Láº¥y columns
    try:
        cur.execute(f"PRAGMA table_info({table_name})")
        columns_info = cur.fetchall()
        if not columns_info:
            print(f"  âŒ Table '{table_name}' not found.")
            conn.close()
            return
        columns = [col[1] for col in columns_info]
    except Exception as e:
        print(f"  âŒ Error reading table info: {e}")
        conn.close()
        return

    # Sá»­ dá»¥ng generator Ä‘á»ƒ duyá»‡t qua dá»¯ liá»‡u, KHÃ”NG load háº¿t vÃ o RAM
    
    # Check if 'approved' column exists
    has_approved = "approved" in [c.lower() for c in columns]
    
    if has_approved:
        sql_query = f"SELECT * FROM {table_name} WHERE approved = 1 OR approved IS NULL"
    else:
        sql_query = f"SELECT * FROM {table_name}"
        
    cur.execute(sql_query)
    
    total_synced = 0
    points_buffer = []

    for rows_chunk in row_generator(cur, batch_size=BATCH_SIZE):
        texts_to_embed = []
        payloads = []
        ids = []

        for row in rows_chunk:
            row_dict = dict(zip(columns, row))
            notion_id = row_dict.get("notion_id")
            
            if not notion_id: continue

            # Build embed text
            text = build_embed_text(row_dict, table_name)
            
            # Táº¡o payload, QUAN TRá»ŒNG: ThÃªm source_table
            payload = {k: v for k, v in row_dict.items() if k != "notion_id"}
            payload["source_table"] = table_name  # Metadata dÃ¹ng Ä‘á»ƒ lá»c sau nÃ y
            
            ids.append(notion_id)
            texts_to_embed.append(text)
            payloads.append(payload)

        # Embed batch nÃ y
        if texts_to_embed:
            try:
                embeddings = embed_model.encode(texts_to_embed, normalize_embeddings=True)
                
                # Táº¡o points
                for i, _id in enumerate(ids):
                    points_buffer.append(PointStruct(
                        id=_id,
                        vector=embeddings[i].tolist(),
                        payload=payloads[i]
                    ))
                
                # Push lÃªn Qdrant ngay khi buffer Ä‘á»§ lá»›n Ä‘á»ƒ giáº£i phÃ³ng RAM
                if len(points_buffer) >= BATCH_SIZE:
                    client.upsert(
                        collection_name=GLOBAL_COLLECTION_NAME,
                        points=points_buffer
                    )
                    total_synced += len(points_buffer)
                    print(f"  Saved {len(points_buffer)} items...", end="\r")
                    points_buffer = []  # Clear buffer
                    gc.collect() # Force clear RAM
                    
            except Exception as e:
                print(f"  âš ï¸ Error embedding batch: {e}")

    # Push ná»‘t nhá»¯ng cÃ¡i cÃ²n sÃ³t láº¡i
    if points_buffer:
        client.upsert(
            collection_name=GLOBAL_COLLECTION_NAME,
            points=points_buffer
        )
        total_synced += len(points_buffer)
    
    conn.close()
    print(f"  âœ… Finished syncing {total_synced} items from '{table_name}'")


def init_global_collection(client):
    """Khá»Ÿi táº¡o collection duy nháº¥t náº¿u chÆ°a cÃ³"""
    try:
        if not client.collection_exists(GLOBAL_COLLECTION_NAME):
            print(f"Creating global collection: {GLOBAL_COLLECTION_NAME}")
            client.create_collection(
                collection_name=GLOBAL_COLLECTION_NAME,
                vectors_config=VectorParams(size=1024, distance=Distance.COSINE),
            )
            # Táº¡o Payload Index cho source_table Ä‘á»ƒ filter nhanh
            client.create_payload_index(
                collection_name=GLOBAL_COLLECTION_NAME,
                field_name="source_table",
                field_schema="keyword"
            )
            print("âœ… Collection & Index created.")
        else:
            print(f"Existing collection found: {GLOBAL_COLLECTION_NAME}")
    except Exception as e:
        print(f"Error checking/creating collection: {e}")

def cleanup_old_collections(client):
    """
    Optional: XÃ³a cÃ¡c collections cÅ© láº» táº» Ä‘á»ƒ dá»n rÃ¡c
    """
    try:
        collections = client.get_collections().collections
        for c in collections:
            if c.name != GLOBAL_COLLECTION_NAME:
                print(f"ğŸ—‘ï¸ Deleting old fragmented collection: {c.name}")
                client.delete_collection(c.name)
    except Exception as e:
        print(f"Warning cleaning up: {e}")

def main():
    print("ğŸš€ Enterprise Sync Started (Memory Safe Mode)...")
    
    # 1. Load Model
    print("ğŸ“¦ Loading embedding model...")
    try:
        embed_model = SentenceTransformer("BAAI/bge-m3", device="cpu") # Force CPU náº¿u GPU yáº¿u
    except:
        embed_model = SentenceTransformer("keepitreal/vietnamese-sbert")
    
    # 2. Connect Qdrant
    print(f"ğŸ”— Connecting to Qdrant ({QDRANT_URL})...")
    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)
    
    # 3. Init Global Collection
    init_global_collection(client)
    
    # 4. Clean up old mess (Theo yÃªu cáº§u clean up Ä‘á»ƒ chuyá»ƒn architecture)
    # cleanup_old_collections(client) # Uncomment náº¿u muá»‘n tá»± Ä‘á»™ng xÃ³a cÃ¡i cÅ©
    
    # 5. Get all tables
    conn = get_db_connection()
    cur = conn.cursor()
    cur.execute("SELECT name FROM sqlite_master WHERE type='table' ORDER BY name")
    all_tables = [row[0] for row in cur.fetchall() if not row[0].startswith("sqlite_")]
    conn.close()
    
    specific_table = sys.argv[1] if len(sys.argv) > 1 else None
    
    if specific_table:
        sync_table_to_global_collection(specific_table, embed_model, client)
    else:
        print(f"ğŸ“‹ Found {len(all_tables)} tables to sync.")
        for table in all_tables:
            sync_table_to_global_collection(table, embed_model, client)
            gc.collect() # Dá»n RAM sau má»—i báº£ng
            
    print("\nğŸ‰ GLOBAL SYNC COMPLETED SUCCESSFULLY!")

if __name__ == "__main__":
    main()
